# 데이터 분석, 머신러닝, 딥러닝 종합 실습 문제 - 개선본

> **작성일:** 2025년 10월 27일  
> **목적:** 학습자가 명확하게 문제를 이해하고 단계적으로 풀 수 있도록 상세한 지시사항과 검증된 소스코드 제공

## 목차
- [Section 1: 데이터 분석 기초](#section-1-데이터-분석-기초)
- [Section 2: 데이터 전처리](#section-2-데이터-전처리)
- [Section 3: 데이터 시각화](#section-3-데이터-시각화)
- [Section 4: 머신러닝 알고리즘](#section-4-머신러닝-알고리즘)
- [Section 5: 딥러닝 및 NLP](#section-5-딥러닝-및-nlp)
- [Section 6: 모델 평가](#section-6-모델-평가)

---

## Section 1: 데이터 분석 기초

### 문제 1.1: NumPy를 활용한 벡터 및 행렬 연산

**💡 학습 목표:**
이 문제를 통해 NumPy 라이브러리를 사용하여 벡터와 행렬의 기본 연산을 수행할 수 있습니다.
- 두 벡터의 내적 계산과 의미 이해
- 행렬 곱셈의 규칙과 결과 해석
- 벡터의 크기(노름)를 다양한 방법으로 계산
- 조건부 인덱싱을 사용한 데이터 필터링

**📋 상세 지시사항:**

1. **벡터 내적(Dot Product) 계산**
   - 두 1D 벡터를 생성: `v1 = [1, 2, 3, 4]`, `v2 = [5, 6, 7, 8]`
   - `np.dot()` 함수 또는 `@` 연산자를 사용하여 내적 계산
   - 내적의 정의를 이용하여 수동으로 계산하고 결과 비교
   - 내적의 의미: 두 벡터의 유사도를 나타냄

2. **행렬 곱셈 구현**
   - 2×3 행렬 A와 3×2 행렬 B를 생성
   - `np.matmul()` 또는 `@` 연산자로 행렬 곱셈 수행
   - 결과는 2×2 행렬이 되어야 함 (행렬 곱셈의 규칙)
   - 첫 번째 행의 계산 과정을 단계별로 출력하여 이해 강화

3. **벡터 노름(Norm) 계산**
   - 벡터 `c = [3, 4, 5]`의 다양한 노름 계산:
     - **L1 노름:** `|3| + |4| + |5| = 12` (맨하탄 거리)
     - **L2 노름:** $\sqrt{3^2 + 4^2 + 5^2} \approx 7.07$ (유클리드 거리, 가장 일반적)
     - **무한대 노름:** `max(3, 4, 5) = 5` (최댓값)
   - `np.linalg.norm()` 함수 사용
   - 수동 계산으로 L2 노름 검증

4. **조건부 인덱싱**
   - 배열 `data = [1, 5, 3, 8, 2, 9, 4, 7, 6]`를 생성
   - 5보다 큰 값 찾기: `data[data > 5]`
   - 짝수 값만 추출: `data[data % 2 == 0]`
   - 범위 조건: 3 이상 7 이하 값만 추출
   - 조건에 맞는 값을 다른 값으로 변경

**💾 파일명:** `problem_1_1_numpy_operations.py`



**🔍 기대 출력:**
```
=== 문제 1.1: NumPy를 활용한 벡터 및 행렬 연산 ===

=== 1. 벡터 내적 계산 ===
벡터 A: [1 2 3 4]
벡터 B: [5 6 7 8]
내적 (np.dot): 70
내적 (@연산자): 70
수동 계산 검증: 70
...
```

---

### 문제 1.2: NumPy 브로드캐스팅 이해 및 활용

**💡 학습 목표:**
NumPy의 브로드캐스팅 메커니즘을 이해하고, 서로 다른 크기의 배열을 효율적으로 연산합니다.

**📋 상세 지시사항:**

1. **브로드캐스팅 규칙 이해**
   - 규칙 1: 스칼라와 배열 연산
     ```python
     arr = np.array([1, 2, 3, 4])
     result = arr + 10  # 각 요소에 10을 더함
     ```
   - 규칙 2: 1D와 2D 배열 연산
     ```python
     arr2d = np.array([[1,2,3], [4,5,6], [7,8,9]])
     arr1d = np.array([10, 20, 30])
     result = arr2d + arr1d  # 각 행에 1D 배열 더함
     ```
   - 규칙 3: 차원 확장
     ```python
     arr_col = np.array([[1], [2], [3]])  # 3x1
     arr_row = np.array([10, 20, 30, 40])  # 1D (1x4)
     result = arr_col + arr_row  # 3x4 결과
     ```

2. **실제 응용 예제**
   - 이미지 데이터 정규화: 각 채널별 평균값 제거
   - 거리 행렬 계산: 포인트 간의 유클리드 거리
   - 온도 데이터 변환: 시간대별 보정값 적용

3. **성능 비교**
   - 브로드캐스팅 사용 vs 반복문 사용
   - 1000×1000 배열로 성능 비교: 반복문 대비 10배 이상 빠름

4. **시각화**
   - 브로드캐스팅 결과의 2D/3D 플롯
   - 거리 행렬 히트맵
   - 성능 벤치마크 그래프

**💾 파일명:** `problem_1_2_numpy_broadcasting.py`



---

### 문제 1.3: Pandas DataFrame 조작 및 데이터 분석

**💡 학습 목표:**
Pandas DataFrame을 이용하여 실제 데이터를 로드, 조작, 분석합니다.

**📋 상세 지시사항:**

1. **DataFrame 생성 및 기본 정보 확인**
   - 딕셔너리로부터 DataFrame 생성 (8명의 직원 데이터)
   - 속성: 이름, 나이, 도시, 급여, 부서, 경력
   - `df.info()`, `df.describe()` 사용하여 데이터 개요 파악

2. **단일 조건 필터링**
   - 나이가 30 이상인 직원 추출
   - 특정 도시(예: 서울) 거주 직원 찾기
   - 급여가 4000 이상인 직원 필터링

3. **복합 조건 필터링**
   - AND 연산: 서울 거주 **AND** 급여 4000 이상
   - OR 연산: 개발팀 **OR** 급여 4500 이상
   - NOT 연산: 개발팀이 아닌 직원

4. **기본 통계 및 집계**
   - 부서별 평균 급여 계산
   - 도시별 직원 수
   - 경력 통계 분석

**💾 파일명:** `problem_1_3_pandas_dataframe.py`



---

### 문제 1.4: Pandas GroupBy를 활용한 집계 데이터 분석

**💡 학습 목표:**
GroupBy를 사용하여 그룹별 집계 분석과 다중 조건 분석을 수행합니다.

**📋 상세 지시사항:**

1. **Titanic 데이터셋 이해**
   - seaborn 라이브러리로 titanic 데이터 로드: `sns.load_dataset('titanic')`
   - 데이터 형태: 891행, 15개 열
   - 주요 열: PassengerId, Survived, Pclass, Age, Sex, Fare, Embarked 등
   - 결측값 확인: Age(177개), Cabin(687개), Embarked(2개) 결측

2. **좌석 등급별 생존율 분석**
   - `groupby('pclass')['survived'].mean()` 사용
   - 예상 결과: 1등석 > 2등석 > 3등석 생존율
   - 생존율 차이 해석

3. **성별-좌석등급별 다중 집계**
   - `groupby(['sex', 'pclass']).agg()` 사용
   - 각 그룹별 나이 평균, 요금 최대값, 생존율 계산
   - 가장 높은 생존율 그룹 찾기

4. **피벗 테이블 생성**
   - 성별 × 좌석등급 생존율 피벗 테이블
   - 히트맵으로 시각화하여 패턴 확인

5. **시각화**
   - 좌석등급별 생존율 막대 그래프
   - 성별-좌석등급 생존율 히트맵
   - 연령대별 생존자 수

**💾 파일명:** `problem_1_4_pandas_groupby.py`




---

## Section 2: 데이터 전처리

### 문제 2.1: 결측값 식별 및 처리 전략 비교

**💡 학습 목표:**
결측값의 패턴을 분석하고 다양한 처리 전략을 비교합니다.

**📋 상세 지시사항:**

1. **결측값 분석**
   - Titanic 데이터의 결측값 시각화
   - 각 열별 결측값 개수와 비율 계산
   - Age 열: 177개 결측 (19.9%)
   - Cabin 열: 687개 결측 (77.1%)

2. **전략 1: 삭제(Deletion)**
   - `df.dropna(subset=['age'])` 사용
   - 891행 → 714행 (177행 삭제)
   - 데이터 손실률: 19.9%
   - 장점: 데이터 품질 유지
   - 단점: 정보 손실

3. **전략 2: 대체(Imputation) - 통계값 사용**
   - 평균값 대체: `age.fillna(age.mean())`
   - 중앙값 대체: `age.fillna(age.median())`
   - 최빈값 대체: `age.fillna(age.mode()[0])`
   - 장점: 데이터 크기 유지
   - 단점: 분포 왜곡 가능

4. **전략 3: 고급 대체 - 그룹별 대체**
   - 성별-좌석등급별 평균값으로 대체
   - `groupby(['sex', 'pclass']).transform(lambda x: x.fillna(x.mean()))`
   - 더 정교한 대체 가능
   - 각 그룹별 평균값 출력

5. **결과 비교**
   - 각 전략별 결측값 개수 확인
   - 분포 변화 시각화
   - 통계값 비교

**💾 파일명:** `problem_2_1_missing_values.py`



---

### 문제 2.2: 이상치 탐지 및 처리

**💡 학습 목표:**
이상치를 여러 방법으로 탐지하고 처리합니다.

**📋 상세 지시사항:**

1. **IQR 방법 (Interquartile Range)**
   - 1사분위수 (Q1), 3사분위수 (Q3) 계산
   - IQR = Q3 - Q1
   - 이상치 범위: Q1 - 1.5×IQR ~ Q3 + 1.5×IQR
   - 범위 밖의 값을 이상치로 분류

2. **Z-score 방법**
   - Z-score = (x - mean) / std
   - |Z-score| > 3: 이상치 (극단적)
   - |Z-score| > 2.5: 이상치 (의심)
   - 표준정규분포 가정

3. **이상치 처리**
   - 삭제
   - 경계값으로 대체 (Capping)
   - 중앙값으로 대체

4. **시각화**
   - Box plot으로 이상치 시각화
   - 히스토그램으로 분포 확인

**💾 파일명:** `problem_2_2_outlier_detection.py`



---

### 문제 2.3: 데이터 스케일링 및 정규화

**💡 학습 목표:**
데이터 스케일링의 중요성을 이해하고 여러 방법을 비교합니다.

**📋 상세 지시사항:**

1. **StandardScaler (표준화)**
   - 공식: $x' = \frac{x - mean}{std}$
   - 결과: 평균 0, 표준편차 1
   - 사용: 정규분포 가정, 선형 모델

2. **MinMaxScaler (정규화)**
   - 공식: $x' = \frac{x - min}{max - min}$
   - 범위: [0, 1]
   - 사용: 범위 제한 필요, 신경망

3. **성능 비교**
   - 스케일링 전후 머신러닝 모델 성능 비교
   - 스케일링 필요성 입증

4. **시각화**
   - 스케일링 전후 분포 비교
   - 각 특성별 범위 비교

**💾 파일명:** `problem_2_3_data_scaling.py`



---

## Section 3: 데이터 시각화

### 문제 3.1: Matplotlib 서브플롯 활용

**💡 학습 목표:**
Matplotlib을 사용하여 다양한 차트를 조합한 복합 시각화를 만듭니다.

**📋 상세 지시사항:**

1. **2×2 서브플롯 구성**
   - `plt.subplot(2, 2, 1)` 또는 `plt.subplots(2, 2)`
   - 각 서브플롯에 다른 차트 유형 배치

2. **다양한 차트 타입**
   - 선 그래프 (plot)
   - 막대 그래프 (bar)
   - 산점도 (scatter)
   - 히스토그램 (hist)

3. **축 설정**
   - 제목: `set_title()`
   - 축 레이블: `set_xlabel()`, `set_ylabel()`
   - 눈금 레이블: `set_xticklabels()`

4. **스타일 커스터마이징**
   - 색상 지정
   - 투명도 (alpha)
   - 선 스타일 (linestyle)
   - 범례 추가

**💾 파일명:** `problem_3_1_matplotlib_subplots.py`



---

### 문제 3.2: Seaborn을 활용한 통계 시각화

**💡 학습 목표:**
Seaborn을 사용하여 고급 통계 시각화를 생성합니다.

**📋 상세 지시사항:**

1. **상관관계 히트맵**
   - 데이터의 상관계수 계산
   - `sns.heatmap()` 사용
   - 색상 패턴으로 상관관계 시각화

2. **분포 분석**
   - `sns.distplot()` 또는 `sns.histplot()`: 데이터 분포
   - `sns.kdeplot()`: 커널 밀도 추정

3. **카테고리별 비교**
   - `sns.boxplot()`: 박스 플롯으로 범주별 분포
   - `sns.violinplot()`: 바이올린 플롯

4. **회귀 분석 시각화**
   - `sns.regplot()`: 회귀선이 포함된 산점도
   - 신뢰 구간 표시

**💾 파일명:** `problem_3_2_seaborn_visualization.py`



---

## Section 4: 머신러닝 알고리즘

### 문제 4.1: 선형 회귀 모델 구현 및 평가

**💡 학습 목표:**
선형 회귀 모델을 구축하고 성능을 평가합니다.

**📋 상세 지시사항:**

1. **데이터 준비**
   - California Housing 데이터 로드
   - 특성: 위도, 경도, 주택 연한, 방 수, 침실 수, 인구, 가구당 인원, 중앙값
   - 타겟: 중앙 주택 가격

2. **데이터 분할**
   - 훈련 데이터 80%, 테스트 데이터 20%
   - `train_test_split()` 사용

3. **모델 구축 및 학습**
   - `LinearRegression()` 객체 생성
   - `fit()` 메서드로 훈련
   - 회귀 계수 확인

4. **예측 및 평가**
   - MSE (Mean Squared Error): $\frac{1}{n}\sum(y - \hat{y})^2$
   - RMSE (Root Mean Squared Error): $\sqrt{MSE}$
   - R² (결정 계수): 모델 설명력 (0 ~ 1)

5. **결과 시각화**
   - 실제값 vs 예측값 산점도
   - 잔차 플롯

**💾 파일명:** `problem_4_1_linear_regression.py`



---

### 문제 4.2: 로지스틱 회귀를 활용한 이진 분류

**💡 학습 목표:**
로지스틱 회귀를 사용하여 이진 분류 문제를 해결합니다.

**📋 상세 지시사항:**

1. **데이터 준비**
   - Breast Cancer 데이터 로드 (569개 샘플, 30개 특성)
   - 타겟: 0=악성(Malignant), 1=양성(Benign)
   - 클래스 불균형 확인

2. **데이터 전처리**
   - StandardScaler로 특성 스케일링 (필수)
   - 스케일링 전후 통계 비교

3. **모델 학습**
   - `LogisticRegression(max_iter=3000)` 생성
   - 훈련 데이터로 학습
   - 과대적합 여부 확인

4. **분류 성능 평가**
   - **혼동 행렬 (Confusion Matrix):**
     - TN (True Negative): 실제 음성, 예측 음성 ✓
     - FP (False Positive): 실제 음성, 예측 양성 ✗ (Type I Error)
     - FN (False Negative): 실제 양성, 예측 음성 ✗ (Type II Error - 의료에서 위험)
     - TP (True Positive): 실제 양성, 예측 양성 ✓
   
   - **정밀도 (Precision):** $\frac{TP}{TP+FP}$ = 양성 예측의 정확도
   - **재현율 (Recall):** $\frac{TP}{TP+FN}$ = 실제 양성 중 찾은 비율
   - **F1-score:** 정밀도와 재현율의 조화평균
   - **정확도 (Accuracy):** $\frac{TP+TN}{Total}$

5. **특성 중요도**
   - 로지스틱 회귀 계수 크기로 중요도 판단
   - 상위 10개 중요 특성 시각화

6. **시각화**
   - 혼동 행렬 히트맵
   - ROC 곡선과 AUC

**💾 파일명:** `problem_4_2_logistic_regression.py`



---

### 문제 4.3: 의사결정트리 분류 및 해석

**💡 학습 목표:**
의사결정트리 모델을 구축하고 트리 구조를 분석합니다.

**📋 상세 지시사항:**

1. **모델 구축**
   - `DecisionTreeClassifier()` 생성
   - 최대 깊이 설정: max_depth 조정으로 과대적합 제어

2. **트리 시각화**
   - `plot_tree()` 함수로 트리 구조 시각화
   - 각 노드의 불순도, 샘플 수 확인

3. **특성 중요도 분석**
   - `feature_importances_` 속성
   - 상위 중요 특성 시각화

4. **가지치기 효과**
   - 깊이 다른 여러 모델 성능 비교
   - 정확도 곡선으로 최적 깊이 찾기

**💾 파일명:** `problem_4_3_tree_classification.py`



---

### 문제 4.4: SVM 분류 및 커널 트릭

**💡 학습 목표:**
SVM의 다양한 커널과 비선형 분류를 이해합니다.

**📋 상세 지시사항:**

1. **선형 커널 (Linear)**
   - 선형으로 분리 가능한 데이터에 사용
   - 계산 속도 가장 빠름

2. **RBF 커널 (Radial Basis Function)**
   - 비선형 데이터 분류
   - 가장 많이 사용됨
   - 감마 파라미터 조정으로 결정 경계 조절

3. **다항식 커널 (Polynomial)**
   - 중간 정도의 비선형성

4. **결정 경계 시각화**
   - 2D 데이터로 메시 생성
   - 각 커널별 결정 경계 시각화
   - 경계의 부드러움 비교

5. **하이퍼파라미터 튜닝**
   - C (정규화 파라미터): 오류 허용 정도
   - gamma: 개별 샘플의 영향 범위

**💾 파일명:** `problem_4_4_svm_classification.py`


---

### 문제 4.5: K-Means 클러스터링

**💡 학습 목표:**
K-Means 알고리즘을 이해하고 최적 클러스터 수를 결정합니다.

**📋 상세 지시사항:**

1. **Elbow Method**
   - 클러스터 수를 1부터 10까지 변화
   - 각 K에 대한 관성(Inertia) 계산
   - 관성이 급격히 감소하는 지점 찾기
   - 보통 K=3~4 근처에서 "팔꿈치" 형태

2. **클러스터링 실행**
   - 최적 K 값으로 모델 학습
   - 각 샘플의 클러스터 할당 확인
   - 클러스터 중심 좌표

3. **결과 시각화**
   - 2D 산점도로 클러스터 표시
   - 클러스터 중심 표시
   - 색상으로 구분

4. **클러스터 특성 분석**
   - 각 클러스터의 평균값 계산
   - 클러스터 크기

**💾 파일명:** `problem_4_5_clustering.py`



---

### 문제 4.6: 고객 세분화를 위한 클러스터링

**💡 학습 목표:**
실제 비즈니스 사례인 고객 세분화 문제를 해결합니다.

**📋 상세 지시사항:**

1. **데이터 생성**
   - 가상의 고객 데이터: 나이, 연간 소득, 지출 점수 등

2. **전처리**
   - 스케일링 (StandardScaler 또는 MinMaxScaler)

3. **다양한 클러스터링 알고리즘 비교**
   - K-Means
   - Hierarchical Clustering
   - DBSCAN

4. **세그먼트 특성 분석**
   - 각 세그먼트의 나이 범위
   - 소득 분포
   - 지출 행동 패턴

5. **마케팅 인사이트 도출**
   - 각 세그먼트별 타겟 전략
   - VIP 고객군 식별
   - 저 소비 고객군 식별

**💾 파일명:** `problem_4_6_kmeans_customer_segmentation.py`



---

## Section 5: 딥러닝 및 NLP

### 문제 5.1: 감정 분석 (Sentiment Analysis)

**💡 학습 목표:**
NLP 기초 기술을 이용하여 텍스트 감정 분류를 수행합니다.

**📋 상세 지시사항:**

1. **텍스트 전처리**
   - 소문자 변환
   - 토큰화 (NLTK의 word_tokenize)
   - 불용어 제거 (stopwords)
   - 특수문자 제거

2. **특성 추출: TF-IDF**
   - Term Frequency (TF): 문서 내 단어 빈도
   - Inverse Document Frequency (IDF): 문서 역빈도
   - TF-IDF = TF × IDF
   - `TfidfVectorizer` 사용

3. **분류 모델**
   - Naive Bayes, SVM, 또는 Logistic Regression
   - 긍정/부정 이진 분류

4. **성능 평가**
   - 정확도, 정밀도, 재현율
   - 혼동 행렬

**💾 파일명:** `problem_5_1_sentiment_analysis.py`



**📥 설치 명령어:**
```bash
pip install scikit-learn pandas nltk
python -m nltk.downloader punkt stopwords
```

---

### 문제 5.2: 손실 함수 및 역전파 구현

**💡 학습 목표:**
신경망의 핵심 개념인 역전파와 경사 하강법을 이해합니다.

**📋 상세 지시사항:**

1. **간단한 신경망 구현**
   - 입력층: 2개 뉴런
   - 은닉층: 4개 뉴런 (ReLU 활성화)
   - 출력층: 1개 뉴런 (시그모이드 활성화)

2. **순전파 (Forward Pass)**
   - 입력 → 은닉층 연산
   - 활성화 함수 적용
   - 은닉층 → 출력층 연산

3. **손실 함수**
   - MSE (평균 제곱 오차)
   - 이진 분류 손실 (Binary Cross-Entropy)

4. **역전파 구현**
   - 연쇄 법칙 (Chain Rule)으로 그래디언트 계산
   - 각 층별 편미분 계산

5. **경사 하강법**
   - 가중치 업데이트: $w' = w - \alpha \cdot \nabla L$
   - 학습률 (learning rate) 영향 분석

6. **학습 과정 시각화**
   - 에포크별 손실값 감소 곡선
   - 가중치 변화

**💾 파일명:** `problem_5_2_loss_backprop.py`



---

### 문제 5.3: 딥러닝을 활용한 감정 분석

**💡 학습 목표:**
LSTM을 사용하여 순차 데이터 감정 분류를 수행합니다.

**📋 상세 지시사항:**

1. **LSTM 네트워크 구성**
   - 임베딩층: 단어를 벡터로 변환
   - LSTM층: 순차 의존성 학습
   - Dense층: 분류 수행

2. **데이터 준비**
   - 영화 리뷰 텍스트 데이터
   - 긍정(1)/부정(0) 라벨
   - 시퀀스 패딩

3. **모델 훈련**
   - 에포크 20~30 정도
   - 배치 크기 32
   - 검증 데이터로 모니터링

4. **성능 평가**
   - 훈련/검증 정확도
   - 손실값 비교
   - 과대적합 여부 확인

5. **예측 및 해석**
   - 새로운 텍스트 감정 예측
   - 확률값 해석

**💾 파일명:** `problem_5_3_deep_sentiment_analysis.py`



**📥 설치 명령어:**
```bash
pip install tensorflow pandas nltk
python -m nltk.downloader punkt stopwords
```

---

## Section 6: 모델 평가

### 문제 6.1: CNN을 활용한 CIFAR-10 이미지 분류

**💡 학습 목표:**
CNN을 구축하여 32×32 컬러 이미지를 10개 클래스로 분류합니다.

**📋 상세 지시사항:**

1. **CIFAR-10 데이터 이해**
   - 60,000개 이미지 (50,000 훈련, 10,000 테스트)
   - 크기: 32×32 픽셀
   - 클래스: 10개 (비행기, 자동차, 새, 고양이, 사슴, 개, 개구리, 말, 배, 트럭)

2. **CNN 모델 구조**
   - Conv2D 층 (필터: 32→64→128)
   - MaxPooling2D 층
   - Flatten 및 Dense 층
   - Dropout (과대적합 방지)

3. **데이터 전처리**
   - 정규화: 픽셀값을 [0, 1]로 스케일링
   - One-hot 인코딩: 라벨 변환

4. **모델 훈련**
   - 옵티마이저: Adam
   - 손실 함수: Categorical Cross-Entropy
   - 에포크 20~50

5. **평가**
   - 테스트 정확도
   - 클래스별 성능 분석

6. **시각화**
   - 훈련/검증 정확도 곡선
   - 훈련/검증 손실 곡선
   - 잘못 분류된 이미지 예시

**💾 파일명:** `problem_6_1_cnn_cifar10.py`



**⏱️ 실행 시간:** 약 5~10분 (GPU 없을 때)

---

### 문제 6.2: 혼동 행렬 기반 평가 지표

**💡 학습 목표:**
분류 모델의 성능을 다양한 지표로 평가합니다.

**📋 상세 지시사항:**

1. **이진 분류 혼동 행렬**
   ```
                예측 음성    예측 양성
   실제 음성    TN (370)     FP (30)
   실제 양성    FN (10)      TP (90)
   ```

2. **파생 지표 계산**
   - **정확도 (Accuracy):** $\frac{TP+TN}{Total} = \frac{460}{500} = 92\%$
   - **정밀도 (Precision):** $\frac{TP}{TP+FP} = \frac{90}{120} = 75\%$ (양성 예측의 정확도)
   - **재현율 (Recall/Sensitivity):** $\frac{TP}{TP+FN} = \frac{90}{100} = 90\%$ (실제 양성 중 찾은 비율)
   - **F1-score:** $2 \times \frac{Precision \times Recall}{Precision + Recall} = 82\%$ (두 지표 조화)
   - **특이도 (Specificity):** $\frac{TN}{TN+FP} = \frac{370}{400} = 92.5\%$ (음성 예측 정확도)

3. **ROC 곡선 및 AUC**
   - ROC: 임계값 변화에 따른 TPR vs FPR
   - AUC: ROC 곡선 아래 면적 (0~1, 1에 가까울수록 좋음)

4. **다중 클래스 평가**
   - 각 클래스별 정밀도, 재현율, F1-score
   - 매크로 평균 vs 가중 평균

5. **시각화**
   - 혼동 행렬 히트맵
   - ROC 곡선
   - 클래스별 성능 비교 그래프

**💾 파일명:** `problem_6_2_confusion_matrix.py`



---

### 문제 6.3: GridSearchCV 하이퍼파라미터 최적화

**💡 학습 목표:**
체계적으로 하이퍼파라미터를 탐색하고 최적값을 찾습니다.

**📋 상세 지시사항:**

1. **Grid Search 개념**
   - 탐색할 파라미터 격자 정의
   - 모든 조합에 대해 모델 훈련
   - 교차 검증으로 성능 평가

2. **예제: SVM 파라미터 최적화**
   - **C** (정규화): [0.1, 1, 10, 100]
   - **kernel** (커널): ['linear', 'rbf', 'poly']
   - **gamma** (RBF 파라미터): ['scale', 'auto', 0.001, 0.01]
   - 총 조합: 4 × 3 × 4 = 48개

3. **교차 검증**
   - k-fold CV (k=5): 데이터를 5개 부분으로 나눔
   - 각 조합에 대해 5회 훈련/평가
   - 평균 성능으로 최적값 결정

4. **최적 파라미터 적용**
   - `best_params_` 속성으로 최적 파라미터 확인
   - `best_score_` 속성으로 최고 점수 확인

5. **결과 시각화**
   - 파라미터 조합별 성능 히트맵
   - 파라미터 영향도 분석

6. **Pipeline을 활용한 자동화**
   - 전처리 + 모델 훈련을 한 번에 처리
   - 리크 (leakage) 방지

**💾 파일명:** `problem_6_3_gridsearch.py`



---

## 📦 설치 및 실행 가이드

### 1단계: 필수 라이브러리 설치

**최소 설치 (Section 1-4):**
```bash
pip install numpy pandas matplotlib seaborn scikit-learn nltk scipy
python -m nltk.downloader punkt stopwords wordnet
```

**전체 설치 (모든 Section 포함):**
```bash
pip install numpy pandas matplotlib seaborn scikit-learn nltk scipy tensorflow keras
python -m nltk.downloader punkt stopwords wordnet
```

### 2단계: 개별 문제 실행

```bash
# 첫 번째 문제 실행
python problem_1_1_numpy_operations.py

# 또는 특정 섹션의 모든 문제 실행
for file in problem_1_*.py; do python "$file"; echo "---"; done
```

### 3단계: 결과 확인

- 터미널 출력에서 결과 확인
- matplotlib 플롯 윈도우에서 그래프 확인
- 각 파일 마지막에 `plt.show()`로 시각화 표시

### 4단계: 코드 수정 및 학습

각 파일을 텍스트 에디터로 열어서:
1. 주석 읽기
2. 코드 라인별 이해
3. 파라미터 변경해보기
4. 결과 분석

---

## ⚠️ 주의사항 및 트러블슈팅

### 1. 한글 폰트 경고
```
UserWarning: Glyph *** missing from font(s)
```
**해결:** 무시하고 진행 가능 (시각화에 영향 없음)

### 2. 외부 데이터 연결 오류
```
ConnectionError: Failed to download data
```
**해결:** 
- 인터넷 연결 확인
- 일부 문제는 오프라인 데이터 사용 가능하도록 대체 구현 포함

### 3. 메모리 부족
```
MemoryError
```
**해결:** 배치 크기 감소 또는 샘플 수 감소
```python
# 샘플 수 감소
data = data.sample(n=10000)  # 10,000개만 사용
```

### 4. GPU 메모리 부족 (딥러닝)
```python
import tensorflow as tf
# GPU 메모리 제한
gpu = tf.config.list_physical_devices('GPU')[0]
tf.config.experimental.set_memory_growth(gpu, True)
```

---

## 📊 학습 로드맵

**초급 (1-2주):**
- Section 1: NumPy 벡터/행렬 연산
- Section 2: Pandas 기본 조작

**중급 (2-3주):**
- Section 3: 데이터 전처리
- Section 4: 데이터 시각화 및 기본 머신러닝

**고급 (3-4주):**
- Section 4: 고급 머신러닝 (SVM, Clustering)
- Section 5-6: 딥러닝 및 모델 평가

---

## ✨ 추가 학습 자료

### 공식 문서
- NumPy: https://numpy.org/doc/
- Pandas: https://pandas.pydata.org/docs/
- Scikit-learn: https://scikit-learn.org/stable/documentation.html
- TensorFlow: https://www.tensorflow.org/guide

### 온라인 코스
- Coursera: Machine Learning (Andrew Ng)
- Fast.ai: Practical Deep Learning
- Kaggle: Learn 섹션

---

## 📝 완성 체크리스트

총 21개 문제 - 완성도 추적

### Section 1 (기초 데이터 분석) - 4개
- [ ] Problem 1.1: NumPy 벡터 및 행렬 연산
- [ ] Problem 1.2: NumPy 브로드캐스팅
- [ ] Problem 1.3: Pandas DataFrame
- [ ] Problem 1.4: Pandas GroupBy

### Section 2 (데이터 전처리) - 3개
- [ ] Problem 2.1: 결측값 처리
- [ ] Problem 2.2: 이상치 탐지
- [ ] Problem 2.3: 데이터 스케일링

### Section 3 (데이터 시각화) - 2개
- [ ] Problem 3.1: Matplotlib 서브플롯
- [ ] Problem 3.2: Seaborn 통계 시각화

### Section 4 (머신러닝 알고리즘) - 6개
- [ ] Problem 4.1: 선형 회귀
- [ ] Problem 4.2: 로지스틱 회귀
- [ ] Problem 4.3: 의사결정트리
- [ ] Problem 4.4: SVM 분류
- [ ] Problem 4.5: K-Means 클러스터링
- [ ] Problem 4.6: 고객 세분화

### Section 5 (딥러닝 및 NLP) - 3개
- [ ] Problem 5.1: 감정 분석 기초
- [ ] Problem 5.2: 역전파 구현
- [ ] Problem 5.3: LSTM 감정 분석

### Section 6 (모델 평가) - 3개
- [ ] Problem 6.1: CNN CIFAR-10
- [ ] Problem 6.2: 혼동 행렬 평가
- [ ] Problem 6.3: GridSearch 최적화

---

## 📞 피드백 및 개선

이 실습 자료에 대한 피드백은 언제든 환영합니다.
- 오류 발견 시 수정 요청
- 추가 예제 제안
- 설명 명확화 요청

---

**Last Updated:** 2025년 10월 27일
**Status:** ✅ 완성 (모든 파일 검증 완료)
