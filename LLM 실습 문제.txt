
문제 1.1: 텍스트 전처리 파이프라인 구축

지시사항:
구두점, 대소문자, 불용어(stopwords)가 포함된 원시 텍스트 문장 리스트가 주어졌을 때, NLTK 라이브러리를 사용하여 토큰화, 소문자 변환, 구두점 제거, 불용어 제거를 수행하는 Python 함수를 작성하세요.

제공 데이터:
raw_texts = [ "Hello everyone, this is the first document for our NLP task!", "We are learning about Natural Language Processing, which is very exciting.", "Preprocessing text is an important and fundamental step." ]

모범 답안 코드:

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

# NLTK 데이터 다운로드 (최초 1회 실행 필요)
try:
    stopwords.words('english')
except LookupError:
    nltk.download('stopwords')
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

def preprocess_text(text_list):
    """
    텍스트 리스트를 받아 전처리 파이프라인을 적용하는 함수.
    :param text_list: 원시 텍스트 문자열의 리스트
    :return: 전처리된 토큰들의 리스트 (문장별로 나뉨)
    """
    # 영어 불용어 세트 로드
    stop_words = set(stopwords.words('english'))
    
    processed_texts =
    for text in text_list:
        # 1. 소문자 변환
        text = text.lower()
        
        # 2. 토큰화
        tokens = word_tokenize(text)
        
        # 3. 구두점 제거 및 불용어 제거
        processed_tokens = [
            word for word in tokens 
            if word.isalpha() and word not in stop_words
        ]
        
        processed_texts.append(processed_tokens)
        
    return processed_texts

# 함수 실행 및 결과 확인
preprocessed_data = preprocess_text(raw_texts)
print(preprocessed_data)

실행 결과 및 해설:

[['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog'], ['journey', 'thousand', 'miles', 'begins', 'single', 'step'], ['question']]

위 코드는 NLP 작업의 가장 기본적이면서도 필수적인 전처리 과정을 보여줍니다. lower()를 통해 대소문자를 통일하여 'JUMPS'와 'jumps'를 같은 단어로 인식하게 하고, word_tokenize로 문장을 단어 단위로 분리합니다. 이후 isalpha()를 통해 구두점과 숫자를 제거하고, stopwords를 제거하여 'the', 'a', 'is'와 같이 문맥적 의미는 적지만 빈번하게 등장하는 단어들을 걸러냅니다. 이 과정을 통해 분석에 불필요한 노이즈를 제거하고 핵심 단어들만 남겨 모델이 데이터의 패턴을 더 효과적으로 학습하도록 돕습니다.
----------------------------------------------------------------------------------------------
문제 1.2: 텍스트에서 벡터로 - TF-IDF

지시사항:
문제 1.1에서 전처리된 텍스트를 사용하여, scikit-learn의 TfidfVectorizer를 적용해 문장들을 TF-IDF 행렬로 변환하세요. 벡터라이저가 학습한 피처(단어) 이름들과 변환된 TF-IDF 행렬을 출력하여 텍스트가 어떻게 수치 데이터로 표현되는지 확인하세요.
제공 데이터:
문제 1.1의 preprocessed_data 결과 사용.
모범 답안 코드:

Python


from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# 문제 1.1의 전처리된 데이터를 다시 문자열 리스트로 변환
# TfidfVectorizer는 토큰화된 리스트가 아닌, 공백으로 구분된 문자열을 입력으로 받습니다.
corpus = [' '.join(tokens) for tokens in preprocessed_data]

# TfidfVectorizer 객체 생성 및 학습
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(corpus)

# 피처 이름 (단어 사전) 확인
feature_names = vectorizer.get_feature_names_out()
print("피처 이름 (어휘 사전):")
print(feature_names)
print("\n" + "="*50 + "\n")

# TF-IDF 행렬을 DataFrame으로 변환하여 가독성 높이기
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=)
print("TF-IDF 행렬:")
print(df_tfidf)
----------------------------------------------------------------------------------------------
문제 1.3: 시맨틱 능력의 발현 - 단어 임베딩 비교
지시사항:
sentence-transformers 라이브러리를 사용하여 사전 훈련된 언어 모델을 로드하세요. ("king", "queen") 쌍과 ("king", "man") 쌍에 대해 각각 코사인 유사도를 계산하고 비교하세요. 마지막으로, "king" - "man" + "woman" 벡터 연산을 수행한 결과가 "queen"의 벡터와 얼마나 유사한지 확인하여, 단어 임베딩이 어떻게 의미적 관계를 학습하는지 확인하세요.
제공 데이터:
sentence-transformers 라이브러리에서 제공하는 사전 훈련 모델 (all-MiniLM-L6-v2)

모범 답안 코드:

Python


# 필요한 라이브러리 설치: pip install sentence-transformers torch
import torch
from sentence_transformers import SentenceTransformer, util

# 1. 사전 훈련된 모델 로드
print("SentenceTransformer 모델 로딩 중...")
model = SentenceTransformer('all-MiniLM-L6-v2')
print("모델 로딩 완료.")

# 2. 단어 임베딩 생성
words = ['king', 'queen', 'man', 'woman']
word_embeddings = model.encode(words, convert_to_tensor=True)

# 단어별 임베딩 딕셔너리 생성
embeddings_dict = {word: emb for word, emb in zip(words, word_embeddings)}

# 3. 코사인 유사도 비교
print("\n" + "="*50)
print("유사도 비교")
print("="*50)

# (king, queen) 유사도
cosine_kq = util.cos_sim(embeddings_dict['king'], embeddings_dict['queen'])
print(f" 유사도 ('king', 'queen'): {cosine_kq.item():.4f}")

# (king, man) 유사도
cosine_km = util.cos_sim(embeddings_dict['king'], embeddings_dict['man'])
print(f" 유사도 ('king', 'man'): {cosine_km.item():.4f}")

# 4. 벡터 연산을 통한 유추(Analogy) 테스트
print("\n" + "="*50)
print("단어 유추 테스트: king - man + woman ≈ queen?")
print("="*50)

# 벡터 연산 수행
result_vector = embeddings_dict['king'] - embeddings_dict['man'] + embeddings_dict['woman']

# 결과 벡터와 'queen' 벡터 간의 유사도 계산
analogy_similarity = util.cos_sim(result_vector, embeddings_dict['queen'])

print(f"연산 결과 벡터와 'queen' 벡터의 유사도: {analogy_similarity.item():.4f}")
print("유사도가 1에 가까울수록 벡터 연산이 의미 관계를 잘 포착했음을 의미합니다.")
----------------------------------------------------------------------------------------------
2장: 자연어 딥러닝의 핵심 개념

문제 2.1: LSTM을 이용한 순차 데이터 모델링 및 감성 분석
지시사항:
PyTorch를 사용하여 간단한 LSTM 기반의 신경망을 구축하고, 주어진 영화 리뷰 데이터셋으로 감성 분류(긍정/부정) 모델을 훈련시키세요. 모델은 단어 인덱스의 시퀀스를 입력받아 이진 분류 결과를 출력해야 합니다.
제공 데이터:
간단한 영화 리뷰 텍스트와 레이블(1: 긍정, 0: 부정)로 구성된 데이터셋.

train_data = {
    "This movie was fantastic and amazing": 1,
    "The acting was terrible and the story was boring": 0,
    "I really enjoyed the plot and the characters": 1,
    "A complete waste of time and money": 0,
    "The visuals were stunning, a true masterpiece": 1,
    "I would not recommend this film to anyone": 0
}

모범 답안 코드:

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from collections import Counter

# --- 1. 데이터 준비 ---
texts = list(train_data.keys())
labels = list(train_data.values())

# 단어 사전 구축
words = ' '.join(texts).lower().split()
word_counts = Counter(words)
vocab = sorted(word_counts, key=word_counts.get, reverse=True)
word_to_int = {word: i+1 for i, word in enumerate(vocab)} # 0은 패딩용으로 남겨둠

# 텍스트를 정수 시퀀스로 변환
text_ints =
for text in texts:
    text_ints.append([word_to_int[word] for word in text.lower().split()])

# 패딩 처리 (모든 시퀀스 길이를 통일)
seq_length = max(len(x) for x in text_ints)
features = torch.zeros((len(text_ints), seq_length), dtype=torch.long)
for i, row in enumerate(text_ints):
    features[i, -len(row):] = torch.tensor(row)[:seq_length]

labels_tensor = torch.tensor(labels, dtype=torch.float32).view(-1, 1)

# --- 2. LSTM 모델 정의 ---
class SentimentLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=0.5)
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, _ = self.lstm(embedded)
        # 마지막 타임스텝의 출력만 사용
        lstm_out = lstm_out[:, -1, :]
        out = self.dropout(lstm_out)
        out = self.fc(out)
        sig_out = self.sigmoid(out)
        return sig_out

# --- 3. 모델 훈련 ---
vocab_size = len(word_to_int) + 1
embedding_dim = 50
hidden_dim = 64
output_dim = 1
n_layers = 2

model = SentimentLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers)
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 30
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    
    output = model(features)
    loss = criterion(output, labels_tensor)
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 5 == 0:
        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')

# --- 4. 모델 테스트 ---
model.eval()
with torch.no_grad():
    test_text = "The movie was good and enjoyable"
    test_int = [word_to_int.get(word, 0) for word in test_text.lower().split()]
    test_feature = torch.zeros((1, seq_length), dtype=torch.long)
    test_feature[0, -len(test_int):] = torch.tensor(test_int)
    
    prediction = model(test_feature)
    print(f"\nTest Text: '{test_text}'")
    print(f"Prediction Score: {prediction.item():.4f}")
    print(f"Predicted Sentiment: {'Positive' if prediction.item() > 0.5 else 'Negative'}")
----------------------------------------------------------------------------------------------
문제 2.2: BERT를 활용한 전이학습의 위력

지시사항:
문제 2.1과 동일한 감성 분류 작업을, 직접 모델을 구축하는 대신 Hugging Face transformers 라이브러리의 사전 훈련된 BERT 모델과 pipeline API를 사용하여 수행하세요. 코드의 간결성과 (개념적인) 성능을 LSTM 모델과 비교해보세요.

제공 데이터:

test_texts = [ 
"I am absolutely thrilled with the results!", # 매우 긍정적 
"This is the worst experience I have ever had.", # 매우 부정적 
"The movie was okay, but I probably wouldn't watch it again.", # 중립/미묘함 
"Despite the long wait, the food was incredibly delicious.", # 복합적 감정 
"The weather today is sunny and pleasant.", # 긍정적 
"I'm feeling a bit down today.", # 부정적 
"This document contains the quarterly financial report." # 중립/사실적 
]

모범 답안 코드:

# transformers 라이브러리 설치 필요: pip install transformers torch
from transformers import pipeline

# 감성 분석 파이프라인 로드
# 모델을 명시하지 않으면 기본 모델(distilbert-base-uncased-finetuned-sst-2-english)이 로드됩니다.
sentiment_pipeline = pipeline("sentiment-analysis")

# 테스트 문장들에 대해 감성 분석 수행
results = sentiment_pipeline(test_texts)

# 결과 출력
for text, result in zip(test_texts, results):
    print(f"Text: {text}")
    # Hugging Face의 기본 레이블은 'POSITIVE'/'NEGATIVE'일 수 있습니다.
    label = result['label']
    score = result['score']
    print(f"  -> Predicted Sentiment: {label}, Score: {score:.4f}\n")

----------------------------------------------------------------------------------------------
3장: 초거대 언어 모델의 실제적 활용
문제 3.1: 오픈소스 LLM과의 첫 만남

지시사항:
Hugging Face transformers 라이브러리를 사용하여 작고 가벼운 오픈소스 LLM(distilgpt2 또는 google/gemma-2b)을 로드하세요. 주어진 프롬프트를 모델에 입력하여 텍스트 완성(text completion) 결과를 생성하고 출력하세요.
제공 데이터 (프롬프트):
"The best way to learn a new programming language is"
모범 답안 코드:
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# 사용할 모델 이름 정의
# gemma-2b는 더 크고 성능이 좋지만, 리소스 요구량이 높습니다.
# 가벼운 실습을 위해 distilgpt2를 권장합니다.
model_name = "distilgpt2" 

# 토크나이저와 모델 로드
# 토크나이저는 텍스트를 모델이 이해할 수 있는 숫자(토큰 ID)로 변환합니다.
# 모델은 이 토큰 ID 시퀀스를 입력받아 다음 토큰을 예측합니다.
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 입력 프롬프트
prompt = "The best way to learn a new programming language is"

# 프롬프트를 토큰 ID로 인코딩
inputs = tokenizer(prompt, return_tensors="pt")

# 텍스트 생성
# max_length는 생성될 텍스트의 최대 길이를 제한합니다.
# pad_token_id는 문장 끝을 나타내는 토큰 ID를 설정하여 경고 메시지를 방지합니다.
outputs = model.generate(
    **inputs, 
    max_length=50, 
    pad_token_id=tokenizer.eos_token_id
)

# 생성된 토큰 ID를 다시 텍스트로 디코딩
generated_text = tokenizer.decode(outputs, skip_special_tokens=True)

print("===== 생성된 텍스트 =====")
print(generated_text)
----------------------------------------------------------------------------------------------
문제 3.2: 창의성 제어하기 - 생성 파라미터의 영향

지시사항:
문제 3.1과 동일한 모델과 프롬프트를 사용하되, generate 함수의 파라미터를 변경하며 여러 번 텍스트를 생성하세요. 다음 네 가지 경우의 결과를 비교하고 각 파라미터가 생성 결과에 미치는 영향을 분석하세요.
1.	기본 설정 (Greedy Search)
2.	높은 temperature (예: 1.5)
3.	낮은 temperature (예: 0.5)
4.	top_p 샘플링 (Nucleus Sampling, 예: 0.90)
모범 답안 코드:
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

prompt = "The best way to learn a new programming language is"
inputs = tokenizer(prompt, return_tensors="pt")

# 다양한 파라미터 설정
generation_configs = {
    "1. Default (Greedy Search)": {"max_length": 50},
    "2. High Temperature (1.5)": {"max_length": 50, "temperature": 1.5, "do_sample": True},
    "3. Low Temperature (0.5)": {"max_length": 50, "temperature": 0.5, "do_sample": True},
    "4. Top-p Sampling (0.90)": {"max_length": 50, "top_p": 0.90, "do_sample": True}
}

for name, config in generation_configs.items():
    print(f"===== {name} =====")
    
    # pad_token_id 설정
    config['pad_token_id'] = tokenizer.eos_token_id
    
    outputs = model.generate(**inputs, **config)
    generated_text = tokenizer.decode(outputs, skip_special_tokens=True)
    print(generated_text + "\n")
----------------------------------------------------------------------------------------------
문제 3.3: 간단한 도메인 응용 - 마케팅 카피 생성기

지시사항:
제품 이름과 핵심 기능 리스트를 입력받아, 이를 구조화된 프롬프트로 조합한 뒤 LLM을 사용하여 짧고 매력적인 마케팅 문구를 생성하는 Python 함수를 작성하세요.

제공 데이터:
●	product_name = "QuantumLeap Smartwatch"
●	features = ["AI-powered fitness tracking", "5-day battery life", "Holographic display"]
모범 답안 코드:

import torch
from transformers import pipeline

# 파이프라인을 사용하면 더 간편하게 구현 가능
# text-generation 파이프라인은 generate 함수의 복잡한 설정을 추상화해줍니다.
generator = pipeline('text-generation', model='distilgpt2')

def generate_marketing_copy(product_name, features):
    """
    제품 정보로 마케팅 문구를 생성하는 함수.
    
    :param product_name: 제품 이름 (str)
    :param features: 제품 기능 리스트 (list of str)
    :return: 생성된 마케팅 문구 (str)
    """
    # 기능 리스트를 문자열로 변환
    feature_string = "\n- ".join(features)
    
    # LLM이 역할을 이해하고 원하는 형식의 출력을 내도록 유도하는 프롬프트 구조화
    prompt = f"""
Generate a short, catchy, and persuasive marketing description for a new product.

Product Name: {product_name}
Key Features:
- {feature_string}

Marketing Description:
"""
    
    # 파이프라인을 사용하여 텍스트 생성
    # max_length는 프롬프트를 포함한 전체 길이이므로 넉넉하게 설정
    # num_return_sequences=1은 하나의 결과만 받겠다는 의미
    responses = generator(
        prompt, 
        max_length=150, 
        num_return_sequences=1,
        pad_token_id=generator.tokenizer.eos_token_id
    )
    
    # 생성된 텍스트에서 프롬프트 부분을 제외하고 반환
    generated_text = responses['generated_text']
    # "Marketing Description:" 이후의 텍스트만 추출
    marketing_copy = generated_text.split("Marketing Description:").strip()
    
    return marketing_copy

# 함수 실행 및 결과 확인
product_name = "QuantumLeap Smartwatch"
features = ["AI-powered fitness tracking", "5-day battery life", "Holographic display"]

copy = generate_marketing_copy(product_name, features)
print(f"===== Generated Marketing Copy for {product_name} =====")
print(copy)

----------------------------------------------------------------------------------------------
4장: 고급 프롬프트 엔지니어링 기법 마스터

문제 4.1: 퓨샷(Few-Shot) 인-컨텍스트 러닝

지시사항:
새로운 영화 리뷰의 감성을 분류하는 프롬프트를 작성하세요. 이 프롬프트는 모델에게 작업을 직접 지시하는 대신, 긍정 리뷰 예시 하나와 부정 리뷰 예시 하나를 먼저 보여준 후, 분류해야 할 새로운 리뷰를 제시해야 합니다. 이러한 '퓨샷(few-shot)' 방식을 통해 모델이 추가적인 훈련(fine-tuning) 없이도 주어진 예시를 보고 작업을 학습하도록 유도하세요.
제공 데이터:
●	예시: {"review": "A masterpiece!", "sentiment": "Positive"}, {"review": "Utterly boring.", "sentiment": "Negative"}
●	분류 대상: {"review": "It was a decent film, with some flaws."}
모범 답안 코드:

from transformers import pipeline

# 텍스트 생성 파이프라인 로드
generator = pipeline('text-generation', model='distilgpt2')

# 분류할 새로운 리뷰
new_review = "It was a decent film, with some flaws."

# 퓨샷 프롬프트 구성
# 모델에게 명확한 패턴(Review -> Sentiment)을 보여주는 것이 중요
prompt = f"""
Classify the sentiment of the movie review. The sentiment can be Positive or Negative.

Review: "A masterpiece!"
Sentiment: Positive

Review: "Utterly boring."
Sentiment: Negative

Review: "{new_review}"
Sentiment:""" # 모델이 이어서 채우도록 유도

# 텍스트 생성 실행
responses = generator(
    prompt,
    max_new_tokens=5, # Sentiment 뒤에 한두 단어만 생성하도록 길이 제한
    num_return_sequences=1,
    pad_token_id=generator.tokenizer.eos_token_id
)

# 결과 파싱
full_text = responses['generated_text']
# 마지막 줄의 Sentiment: 이후 텍스트 추출
prediction = full_text.split('Sentiment:')[-1].strip()

print(f"Review to classify: '{new_review}'")
print(f"Predicted Sentiment: {prediction}")


----------------------------------------------------------------------------------------------
문제 4.2: LangChain을 이용한 기본 RAG 시스템 구축

지시사항:
LangChain 라이브러리를 사용하여 간단한 검색 증강 생성(Retrieval-Augmented Generation, RAG) 시스템을 구축하세요. 주어진 텍스트 문서를 기반으로 소규모 벡터 저장소(vector store)를 생성한 후, 해당 문서의 정보로만 답변할 수 있는 질문을 하여 LLM이 외부 지식을 참조해 답변하도록 만드세요.
제공 데이터:
●	문서: "Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass more than two and a half times that of all the other planets in the Solar System combined. Jupiter's largest moon is Ganymede."
●	질문: "What is the name of Jupiter's largest moon?"
모범 답안 코드:

# 필요한 라이브러리 설치:
# pip install langchain langchain-community langchain-huggingface faiss-cpu sentence-transformers
import os
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFaceHub

# 환경 변수 설정 (Hugging Face Hub API 토큰 필요)
# https://huggingface.co/settings/tokens 에서 토큰을 발급받아 아래에 입력하세요.
# os.environ = "YOUR_HF_TOKEN"

# 1. 문서 로드 및 분할
document_text = """
Jupiter is the fifth planet from the Sun and the largest in the Solar System. 
It is a gas giant with a mass more than two and a half times that of all the other planets in the Solar System combined. 
Jupiter's largest moon is Ganymede.
"""
text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
docs = text_splitter.create_documents([document_text])

# 2. 임베딩 모델 및 벡터 저장소 생성
# 임베딩 모델은 텍스트를 벡터로 변환합니다.
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# FAISS는 효율적인 유사도 검색을 위한 벡터 저장소입니다.
vectorstore = FAISS.from_documents(docs, embeddings)

# 3. LLM 및 RAG 체인 설정
# 여기서는 Hugging Face Hub의 무료 모델을 사용합니다.
llm = HuggingFaceHub(repo_id="google/flan-t5-small", model_kwargs={"temperature":0.1})

# RetrievalQA 체인은 검색(retrieval)과 생성(generation)을 결합합니다.
# "stuff" 체인 타입은 검색된 문서를 모두 컨텍스트에 넣어 LLM에 전달합니다.
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# 4. 질문 및 답변 생성
question = "What is the name of Jupiter's largest moon?"
response = qa_chain.invoke(question)

print(f"Question: {question}")
print(f"Answer: {response['result']}")
----------------------------------------------------------------------------------------------
문제 4.3: 사고의 연쇄(Chain-of-Thought)를 통한 추론 유도

지시사항:
간단한 다단계 연산이 필요한 응용 문제를 LLM에게 풀어보게 하세요. 첫 번째 시도에서는 최종 답변만 요구하고, 두 번째 시도에서는 프롬프트에 "Let's think step by step."이라는 문구를 추가하여 모델이 문제 해결 과정을 단계별로 서술하도록 유도하세요. 두 결과의 정확성과 출력 내용을 비교 분석하세요.
제공 데이터:
●	문제: "A farmer has 15 apples. He sells 3 to his neighbor and then buys 5 more. He then divides the apples equally among his 4 children. How many apples does each child get?"
모범 답안 코드:
from transformers import pipeline
import torch

# 추론 작업에는 더 큰 모델이 유리합니다. 리소스가 부족할 경우 'distilgpt2'로 변경할 수 있습니다.
# Gemma 모델 사용 시 Hugging Face 로그인이 필요할 수 있습니다.
generator = pipeline('text-generation', model='google/gemma-2b', torch_dtype=torch.bfloat16, device_map="auto")

problem = "A farmer has 15 apples. He sells 3 to his neighbor and then buys 5 more. He then divides the apples equally among his 4 children. How many apples does each child get?"

# 시도 1: 최종 답변만 요구
prompt1 = f"""
Question: {problem}
Answer:
"""

print("===== Attempt 1: Direct Answer =====")
response1 = generator(prompt1, max_new_tokens=50, pad_token_id=generator.tokenizer.eos_token_id)
print(response1['generated_text'])


# 시도 2: 사고의 연쇄(Chain-of-Thought) 프롬프트
prompt2 = f"""
Question: {problem}
Let's think step by step.
Answer:
"""

print("\n===== Attempt 2: Chain-of-Thought =====")
response2 = generator(prompt2, max_new_tokens=150, pad_token_id=generator.tokenizer.eos_token_id)
print(response2['generated_text'])
----------------------------------------------------------------------------------------------
5장: 파인튜닝을 통한 LLM의 효율적 적응
문제 5.1: PEFT(LoRA)를 이용한 파라미터 효율적 파인튜닝

지시사항:
Hugging Face의 PEFT 라이브러리를 사용하여 사전 훈련된 소형 모델(distilgpt2)에 LoRA(Low-Rank Adaptation)를 적용하세요. 주어진 소규모 맞춤형 데이터셋(법률 질문 -> 답변 형식)으로 모델을 파인튜닝하여, 모델이 법률 용어에 대해 더 적절하게 답변하도록 행동을 변화시키는 과정을 실습하세요.
제공 데이터:
instruction-response 쌍으로 구성된 소규모 JSONL 데이터셋.
(아래 내용을 legal_data.jsonl 파일로 저장하여 사용하세요.)


{"instruction": "What is a tort?", "output": "A tort is a civil wrong that causes a claimant to suffer loss or harm, resulting in legal liability for the person who commits the tortious act."}
{"instruction": "Explain the concept of 'habeas corpus'.", "output": "Habeas corpus is a legal recourse through which a person can report an unlawful detention or imprisonment to a court and request that the court order the custodian of the person, usually a prison official, to bring the prisoner to court to determine if the detention is lawful."}
{"instruction": "What does 'pro bono' mean?", "output": "'Pro bono publico', often shortened to 'pro bono', is a Latin phrase for professional work undertaken voluntarily and without payment. It is a way for professionals to contribute to the community."}

모범 답안 코드:


# 필요한 라이브러리 설치:
# pip install transformers datasets peft accelerate bitsandbytes trl
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer

# 1. 데이터셋 로드 및 포맷팅
def formatting_prompts_func(example):
    output_texts =
    for i in range(len(example['instruction'])):
        text = f"### Instruction:\n{example['instruction'][i]}\n\n### Response:\n{example['output'][i]}"
        output_texts.append(text)
    return {"text": output_texts}

dataset = load_dataset("json", data_files="legal_data.jsonl", split="train")
dataset = dataset.map(formatting_prompts_func, batched=True)


# 2. 모델 및 토크나이저 로드
model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token # 패딩 토큰 설정
model = AutoModelForCausalLM.from_pretrained(model_name)

# 3. LoRA 설정
# LoRA는 모델의 특정 레이어(주로 어텐션)에 작은 '어댑터' 행렬을 추가하여 훈련합니다.
# r: LoRA 행렬의 랭크(rank). 낮을수록 파라미터 수가 적어짐.
# lora_alpha: 랭크에 대한 스케일링 팩터.
# target_modules: LoRA를 적용할 모델의 모듈 이름.
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["c_attn", "c_proj"], # GPT-2의 어텐션 관련 레이어
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)
peft_model = get_peft_model(model, lora_config)
peft_model.print_trainable_parameters() # 훈련 가능한 파라미터 수 확인

# 4. 훈련 설정 및 실행
training_args = TrainingArguments(
    output_dir="./lora_results",
    per_device_train_batch_size=1,
    num_train_epochs=10,
    logging_steps=1,
    learning_rate=2e-4,
)

trainer = SFTTrainer(
    model=peft_model,
    train_dataset=dataset,
    dataset_text_field="text",
    args=training_args,
    max_seq_length=128,
)

trainer.train()

# 5. 파인튜닝된 모델 테스트
prompt = "### Instruction:\nWhat is a tort?\n\n### Response:\n"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = peft_model.generate(**inputs, max_new_tokens=60)
print("\n===== 파인튜닝 후 모델 응답 =====")
print(tokenizer.decode(outputs, skip_special_tokens=True))
----------------------------------------------------------------------------------------------
문제 5.2: 개념적 RLHF - 보상 모델의 역할 이해

지시사항:
이 문제는 코드 작성과 개념 이해를 결합한 문제입니다. LLM이 하나의 프롬프트에 대해 생성한 두 개의 다른 응답이 주어졌을 때, '보상 모델(Reward Model)'의 역할을 하는 간단한 Python 함수를 작성하세요. 이 함수는 정교한 AI 모델이 아니라, 길이, 특정 키워드 포함 여부, 공손함 등 간단한 휴리스틱(heuristic) 규칙을 기반으로 어떤 응답이 '더 나은지' 판단하고 더 높은 점수를 반환해야 합니다.
제공 데이터:
●	프롬프트: "Explain photosynthesis."
●	응답 A: "It's how plants eat."
●	응답 B: "Photosynthesis is the process used by plants, algae, and certain bacteria to harness energy from sunlight and turn it into chemical energy."
모범 답안 코드:

def simple_reward_model(prompt, response_a, response_b):
    """
    간단한 휴리스틱 기반의 보상 모델 함수.
    더 나은 응답에 높은 점수를 부여한다.
    
    :param prompt: 원본 프롬프트
    :param response_a: 첫 번째 응답
    :param response_b: 두 번째 응답
    :return: (응답 A 점수, 응답 B 점수, 더 나은 응답 문자열)
    """
    score_a = 0
    score_b = 0
    
    # 휴리스틱 1: 더 길고 상세한 응답에 가산점
    if len(response_a) > len(response_b):
        score_a += 1
    elif len(response_b) > len(response_a):
        score_b += 1
        
    # 휴리스틱 2: 전문 용어 포함 여부에 가산점
    keywords = ["process", "energy", "sunlight", "chemical"]
    for keyword in keywords:
        if keyword in response_a.lower():
            score_a += 0.5
        if keyword in response_b.lower():
            score_b += 0.5
            
    # 휴리스틱 3: 지나치게 짧은 응답에 감점
    if len(response_a.split()) < 5:
        score_a -= 2
    if len(response_b.split()) < 5:
        score_b -= 2
        
    # 최종 판단
    if score_a > score_b:
        preferred_response = "Response A"
    elif score_b > score_a:
        preferred_response = "Response B"
    else:
        preferred_response = "Tie"
        
    return score_a, score_b, preferred_response

# 함수 실행 및 결과 확인
prompt = "Explain photosynthesis."
response_a = "It's how plants eat."
response_b = "Photosynthesis is the process used by plants, algae, and certain bacteria to harness energy from sunlight and turn it into chemical energy."

score_a, score_b, winner = simple_reward_model(prompt, response_a, response_b)

print(f"Prompt: '{prompt}'")
print(f"Response A: '{response_a}' (Score: {score_a})")
print(f"Response B: '{response_b}' (Score: {score_b})")
print(f"==> Preferred Response: {winner}")
----------------------------------------------------------------------------------------------
6장: 멀티모달 AI의 최전선 탐험
문제 6.1: 이미지 캡셔닝

지시사항:
Hugging Face transformers 라이브러리에서 사전 훈련된 이미지 캡셔닝 모델(예: ViT-GPT2)을 사용하세요. 주어진 이미지 URL을 모델에 입력하여, 이미지를 설명하는 자연어 캡션을 생성하고 출력하세요.

제공 데이터:
●	이미지 URL: http://images.cocodataset.org/val2017/000000039769.jpg
모범 답안 코드:

Python


# 필요한 라이브러리 설치: pip install transformers torch Pillow requests
import requests
from PIL import Image
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
import torch

# 이미지 캡셔닝을 위한 사전 훈련된 모델과 관련 컴포넌트 로드
# VisionEncoderDecoderModel은 이미지 인코더(ViT)와 텍스트 디코더(GPT-2)를 결합한 구조
model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
# ImageProcessor는 이미지를 모델이 처리할 수 있는 픽셀 값 텐서로 변환
feature_extractor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
# Tokenizer는 텍스트 디코더가 사용할 토크나이저
tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# 이미지 로드
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# 이미지 전처리 및 캡션 생성
# 이미지를 픽셀 값으로 변환
pixel_values = feature_extractor(images=image, return_tensors="pt").pixel_values.to(device)

# 모델을 사용하여 캡션 토큰 ID 생성
generated_ids = model.generate(pixel_values, max_length=16, num_beams=4)

# 토큰 ID를 텍스트로 디코딩
preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
preds = [pred.strip() for pred in preds]

print("===== 생성된 이미지 캡션 =====")
print(preds)
----------------------------------------------------------------------------------------------
문제 6.2: 확산 모델(Diffusion Model)을 이용한 텍스트-이미지 생성

지시사항:
Hugging Face의 diffusers 라이브러리를 사용하여 사전 훈련된 텍스트-이미지 생성 모델(예: Stable Diffusion)을 로드하세요. 이미지를 설명하는 상세한 텍스트 프롬프트를 작성하고, 이를 바탕으로 새로운 이미지를 생성하세요. 프롬프트의 내용을 변경하며 생성되는 이미지가 어떻게 달라지는지 실험해보세요.
제공 데이터 (프롬프트):
"An astronaut riding a horse on Mars, photorealistic style."

모범 답안 코드:


# 필요한 라이브러리 설치: pip install diffusers transformers accelerate torch
import torch
from diffusers import StableDiffusionPipeline

# Stable Diffusion 파이프라인 로드
# torch_dtype=torch.float16은 GPU 메모리 사용량을 줄여줍니다.
# 이 코드는 CUDA 지원 GPU가 있는 환경에서 실행하는 것을 강력히 권장합니다.
model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe = pipe.to("cuda") # 파이프라인을 GPU로 이동

# 이미지 생성을 위한 프롬프트
prompt = "An astronaut riding a horse on Mars, photorealistic style."

# 프롬프트를 기반으로 이미지 생성
# num_inference_steps는 노이즈 제거 단계 수. 높을수록 품질이 좋아지지만 시간이 오래 걸림.
image = pipe(prompt, num_inference_steps=50).images

# 생성된 이미지 저장 및 확인
image.save("astronaut_on_mars.png")
print("이미지가 'astronaut_on_mars.png' 파일로 저장되었습니다.")
# (Colab, Jupyter 환경에서는 image 객체를 바로 출력하여 확인 가능)
# from IPython.display import display
# display(image)
