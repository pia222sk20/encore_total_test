# LLM ì‹¤ìŠµ ë¬¸ì œ 

---

## ëª©ì°¨

- [Chapter 1: í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ í‘œí˜„](#chapter-1-í…ìŠ¤íŠ¸-ë°ì´í„°ì˜-í‘œí˜„)
- [Chapter 2: ìì—°ì–´ ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ ê°œë…](#chapter-2-ìì—°ì–´-ë”¥ëŸ¬ë‹ì˜-í•µì‹¬-ê°œë…)
- [Chapter 3: ì´ˆê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ ì‹¤ì œì  í™œìš©](#chapter-3-ì´ˆê±°ëŒ€-ì–¸ì–´-ëª¨ë¸ì˜-ì‹¤ì œì -í™œìš©)
- [Chapter 4: ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²•](#chapter-4-ê³ ê¸‰-í”„ë¡¬í”„íŠ¸-ì—”ì§€ë‹ˆì–´ë§-ê¸°ë²•)
- [Chapter 5: íŒŒì¸íŠœë‹ì„ í†µí•œ LLMì˜ íš¨ìœ¨ì  ì ì‘](#chapter-5-íŒŒì¸íŠœë‹ì„-í†µí•œ-llmì˜-íš¨ìœ¨ì -ì ì‘)
- [Chapter 6: ë©€í‹°ëª¨ë‹¬ AIì˜ ìµœì „ì„ ](#chapter-6-ë©€í‹°ëª¨ë‹¬-aiì˜-ìµœì „ì„ )

---

## Chapter 1: í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ í‘œí˜„

### ë¬¸ì œ 1.1: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ ê¸°ê³„ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ê¹¨ë—í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ë°°ì›ë‹ˆë‹¤. ìì—°ì–´ ì²˜ë¦¬ì˜ ê°€ì¥ ì²« ë²ˆì§¸ ë‹¨ê³„ì´ë©°, ë°ì´í„°ì˜ í’ˆì§ˆì´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í¬ê²Œ ì¢Œìš°í•©ë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **NLTK ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´í•´**
   - NLTK: Natural Language Toolkit - ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
   - í† í¬ë‚˜ì´ì €, ë¶ˆìš©ì–´, ë‹¤ì–‘í•œ NLP ë„êµ¬ í¬í•¨
   - ì²« ì‹¤í–‰ ì‹œ í•„ìš”í•œ ë°ì´í„° ìë™ ë‹¤ìš´ë¡œë“œ í•„ìš”

2. **í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë‹¨ê³„ë³„ êµ¬í˜„**

   **Step 1: ì›ì‹œ ë¬¸ì¥ ì…ë ¥**
   ```python
   text = "Hello everyone, this is the first document for our NLP task!"
   ```

   **Step 2: ì†Œë¬¸ì ë³€í™˜**
   ```python
   lower_text = text.lower()
   # ê²°ê³¼: "hello everyone, this is the first document for our nlp task!"
   # ì´ìœ : ê°™ì€ ë‹¨ì–´ë¥¼ 'Hello'ì™€ 'hello'ë¡œ ì¤‘ë³µ ì¸ì‹í•˜ì§€ ì•Šê¸° ìœ„í•¨
   ```

   **Step 3: í† í°í™” (ë‹¨ì–´ ë¶„ë¦¬)**
   ```python
   from nltk.tokenize import word_tokenize
   tokens = word_tokenize(lower_text)
   # ê²°ê³¼: ['hello', 'everyone', ',', 'this', 'is', 'the', 'first', 'document', 'for', 'our', 'nlp', 'task', '!']
   # ì£¼ì˜: êµ¬ë‘ì ë„ ë³„ë„ì˜ í† í°ìœ¼ë¡œ ë¶„ë¦¬ë¨
   ```

   **Step 4: êµ¬ë‘ì  ì œê±°**
   ```python
   # ì•ŒíŒŒë²³ ë¬¸ìë§Œ ë‚¨ê¸°ê¸°: isalpha() ë©”ì„œë“œ ì‚¬ìš©
   alphabetic_tokens = [word for word in tokens if word.isalpha()]
   # ê²°ê³¼: ['hello', 'everyone', 'this', 'is', 'the', 'first', 'document', 'for', 'our', 'nlp', 'task']
   ```

   **Step 5: ë¶ˆìš©ì–´(Stopwords) ì œê±°**
   ```python
   from nltk.corpus import stopwords
   stop_words = set(stopwords.words('english'))
   # ë¶ˆìš©ì–´: 'the', 'is', 'a', 'for' ë“± ë¹ˆë²ˆí•˜ì§€ë§Œ ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ë“¤
   
   filtered_tokens = [word for word in alphabetic_tokens if word not in stop_words]
   # ê²°ê³¼: ['hello', 'everyone', 'first', 'document', 'nlp', 'task']
   # 13ê°œ í† í° â†’ 6ê°œ í† í°ìœ¼ë¡œ 53% ì¶•ì†Œ
   ```

3. **ì „ì²´ ë‹¨ê³„ ì ìš©**
   - ì œê³µëœ 3ê°œ ë¬¸ì¥ì— ëª¨ë‘ ì ìš©
   - ê° ë¬¸ì¥ë³„ ì²˜ë¦¬ ê³¼ì • ì¶œë ¥
   - ìµœì¢… ê²°ê³¼ ë¹„êµ

4. **ë¶„ì„ ë° í†µê³„**
   - ì›ë³¸ ë‹¨ì–´ ìˆ˜ vs ì „ì²˜ë¦¬ í›„ ë‹¨ì–´ ìˆ˜
   - ì œê±°ëœ êµ¬ë‘ì  ëª©ë¡
   - ì œê±°ëœ ë¶ˆìš©ì–´ ëª©ë¡
   - ìµœì¢… ê³ ìœ  ë‹¨ì–´ ëª©ë¡

**ğŸ’¾ íŒŒì¼ëª…:** `llm_1_1_text_preprocessing.py`

**ğŸ” ê¸°ëŒ€ ì¶œë ¥:**
```
=== ë¬¸ì œ 1.1: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ===

=== ì›ë³¸ í…ìŠ¤íŠ¸ ===
1. Hello everyone, this is the first document for our NLP task!
2. We are learning about Natural Language Processing, which is very exciting.
3. Preprocessing text is an important and fundamental step.

=== ì „ì²˜ë¦¬ ê²°ê³¼ ===
1. ['hello', 'everyone', 'first', 'document', 'nlp', 'task']
2. ['learning', 'natural', 'language', 'processing', 'exciting']
3. ['preprocessing', 'text', 'important', 'fundamental', 'step']

=== í†µê³„ ì •ë³´ ===
ì›ë³¸ ì´ ë‹¨ì–´ ìˆ˜: 35
ì „ì²˜ë¦¬ í›„ ë‹¨ì–´ ìˆ˜: 16
ì••ì¶•ë¥ : 54.3%
```

**ğŸ’¡ í•µì‹¬ ê°œë…:**
- **ì™œ ì „ì²˜ë¦¬ê°€ í•„ìš”í•œê°€?** ì›ì‹œ ë°ì´í„°ì—ëŠ” ë…¸ì´ì¦ˆê°€ ë§ì•„ì„œ ëª¨ë¸ì´ ë³¸ì§ˆì ì¸ íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ì–´ë ¤ì›€
- **ë¶ˆìš©ì–´ ì œê±°ì˜ ì¥ì :** ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ, ê³„ì‚° ì†ë„ í–¥ìƒ, ë…¸ì´ì¦ˆ ê°ì†Œ
- **í•œê³„:** ë¬¸ë§¥ì— ë”°ë¼ ë¶ˆìš©ì–´ê°€ ì¤‘ìš”í•  ìˆ˜ ìˆìŒ (ì˜ˆ: "be" ë™ì‚¬)

---

### ë¬¸ì œ 1.2: í…ìŠ¤íŠ¸ì—ì„œ ë²¡í„°ë¡œ - TF-IDF

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ ì»´í“¨í„°ê°€ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ê¸°ê³„í•™ìŠµ ëª¨ë¸ì€ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì´í•´í•˜ì§€ ëª»í•˜ë¯€ë¡œ, ë¨¼ì € ë²¡í„°í™”(vectorization)ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **TF-IDFì˜ ì´í•´**

   **TF (Term Frequency): ë‹¨ì–´ ë¹ˆë„**
   $$TF(w, d) = \frac{\text{ë‹¨ì–´ } w \text{ê°€ ë¬¸ì„œ } d \text{ì— ë“±ì¥í•œ íšŸìˆ˜}}{\text{ë¬¸ì„œ } d \text{ì˜ ì „ì²´ ë‹¨ì–´ ìˆ˜}}$$
   
   ì˜ˆì‹œ: ë¬¸ì„œì— ì´ 100ê°œ ë‹¨ì–´ê°€ ìˆê³ , "machine"ì´ 5ë²ˆ ë‚˜íƒ€ë‚˜ë©´
   - $TF(\text{machine}) = 5/100 = 0.05$

   **IDF (Inverse Document Frequency): ì—­ë¬¸ì„œ ë¹ˆë„**
   $$IDF(w) = \log\left(\frac{\text{ì „ì²´ ë¬¸ì„œ ìˆ˜}}{\text{ë‹¨ì–´ } w \text{ë¥¼ í¬í•¨í•œ ë¬¸ì„œ ìˆ˜}}\right)$$
   
   ì˜ˆì‹œ: ì „ì²´ 1000ê°œ ë¬¸ì„œ ì¤‘ "machine"ì„ í¬í•¨í•œ ë¬¸ì„œê°€ 50ê°œ
   - $IDF(\text{machine}) = \log(1000/50) = \log(20) \approx 2.996$
   
   ì§ê´€: ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” IDFê°€ ë‚®ê³ , ë“œë¬¸ ë‹¨ì–´ëŠ” IDFê°€ ë†’ìŒ

   **TF-IDF**
   $$TF\text{-}IDF(w, d) = TF(w, d) \times IDF(w)$$

2. **ë²¡í„°í™” ê³¼ì •**
   - ë¬¸ì œ 1.1ì˜ ì „ì²˜ë¦¬ëœ 3ê°œ ë¬¸ì¥ ì‚¬ìš©
   - scikit-learnì˜ `TfidfVectorizer` ì ìš©
   - ê²°ê³¼: 3Ã—N í–‰ë ¬ (3ê°œ ë¬¸ì¥, Nê°œ ê³ ìœ  ë‹¨ì–´)

3. **ê²°ê³¼ ë¶„ì„**
   - ê° ë¬¸ì¥ì´ ì–´ë–¤ ë‹¨ì–´ì— ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸
   - ë‹¨ì–´ë³„ IDF ê°’ í™•ì¸
   - ìƒˆë¡œìš´ ë¬¸ì¥ì„ ë²¡í„°í™”í•˜ì—¬ ê¸°ì¡´ ë¬¸ì¥ê³¼ ìœ ì‚¬ë„ ê³„ì‚°

4. **ì˜ë¯¸ í•´ì„**
   - ê³ ìœ í•œ ë‹¨ì–´(íŠ¹ì • ë¬¸ì„œì—ë§Œ ë“±ì¥): TF-IDF ê°’ ë†’ìŒ â†’ ê·¸ ë¬¸ì„œì˜ íŠ¹ì„±ì„ ì˜ ë‚˜íƒ€ëƒ„
   - ê³µí†µ ë‹¨ì–´(ëª¨ë“  ë¬¸ì„œì— ë“±ì¥): TF-IDF ê°’ ë‚®ìŒ â†’ êµ¬ë¶„ë ¥ ë‚®ìŒ
   - ë§¤ìš° í”í•œ ë‹¨ì–´(ê±°ì˜ ëª¨ë“  ë¬¸ì„œì— ë“±ì¥): TF-IDF ê°’ ê±°ì˜ 0

**ğŸ’¾ íŒŒì¼ëª…:** `llm_1_2_tfidf_vectorization.py`


**ğŸ” ê¸°ëŒ€ ì¶œë ¥:**
```
=== ë¬¸ì œ 1.2: í…ìŠ¤íŠ¸ì—ì„œ ë²¡í„°ë¡œ - TF-IDF ===

=== í”¼ì²˜ ì´ë¦„ (ì–´íœ˜ ì‚¬ì „) ===
['document', 'exciting', 'first', 'fundamental', 'important', 'learning', 'nlp', 'preprocessing', 'processing', 'step', 'task', 'text']

=== TF-IDF í–‰ë ¬ ===
             document  exciting  first  ...  task  text
ë¬¸ì„œ 1       0.316228       0.0  0.316228      0.316228
ë¬¸ì„œ 2       0.000000       0.447214  0.000000  0.000000
ë¬¸ì„œ 3       0.000000       0.0  0.000000  0.000000

=== ê° ë‹¨ì–´ì˜ IDF ê°’ ===
        ë‹¨ì–´  IDF ê°’
document    1.609438
exciting    1.609438
first       1.609438
...
```

**ğŸ’¡ í•µì‹¬ ê°œë…:**
- **ì¥ì :** ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì , í¬ê·€ ë‹¨ì–´ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
- **ë‹¨ì :** ë‹¨ì–´ ìˆœì„œ ë¬´ì‹œ (Bag of Words), ì˜ë¯¸ ê´€ê³„ ë°˜ì˜ ì•ˆ í•¨
- **ê°œì„ :** Word2Vec, BERT ë“± ì„ë² ë”© ê¸°ë²•

---

### ë¬¸ì œ 1.3: ì‹œë§¨í‹± ëŠ¥ë ¥ì˜ ë°œí˜„ - ë‹¨ì–´ ì„ë² ë”© ë¹„êµ

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
ë‹¨ì–´ì˜ "ì˜ë¯¸"ë¥¼ ìˆ˜ì¹˜ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ì„ë² ë”©ì˜ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤. ì„ë² ë”©ì€ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì´ ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œì´ ìœ„ì¹˜í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **ë‹¨ì–´ ì„ë² ë”©ì˜ ê°œë…**
   
   **TF-IDF vs Word Embedding ë¹„êµ:**
   ```
   TF-IDF:      ë‹¨ìˆœíˆ ë‹¨ì–´ê°€ ë§ì´ ë“±ì¥í•˜ëŠ”ì§€ ì—¬ë¶€ (ë¹ˆë„ ê¸°ë°˜)
   ì„ë² ë”©:      ë‹¨ì–´ì˜ 'ì˜ë¯¸'ë¥¼ í•™ìŠµ (ì˜ë¯¸ ê¸°ë°˜)
   
   ì˜ˆ: "ì¢‹ë‹¤"ì™€ "í›Œë¥­í•˜ë‹¤"
   - TF-IDF: ì „í˜€ ë‹¤ë¥¸ ë²¡í„° (ë‹¤ë¥¸ ë‹¨ì–´)
   - ì„ë² ë”©: ë§¤ìš° ìœ ì‚¬í•œ ë²¡í„° (ìœ ì‚¬í•œ ì˜ë¯¸)
   ```

2. **ì½”ì‚¬ì¸ ìœ ì‚¬ë„(Cosine Similarity)**
   
   $$\text{cos\_similarity}(v_1, v_2) = \frac{v_1 \cdot v_2}{||v_1|| \times ||v_2||}$$
   
   - ë²”ìœ„: -1 ~ 1 (ë³´í†µ 0 ~ 1)
   - 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡: ê°™ì€ ë°©í–¥ (ë§¤ìš° ìœ ì‚¬)
   - 0: ìˆ˜ì§ (ê´€ë ¨ ì—†ìŒ)
   - -1ì— ê°€ê¹Œìš¸ìˆ˜ë¡: ë°˜ëŒ€ ë°©í–¥ (ì •ë°˜ëŒ€)

3. **ì˜ë¯¸ ìœ ì¶” í…ŒìŠ¤íŠ¸ (Analogy Test)**
   
   **ê°€ì„¤:** ì„ë² ë”©ì´ ì œëŒ€ë¡œ í•™ìŠµë˜ì—ˆë‹¤ë©´
   ```
   king - man + woman â‰ˆ queen
   ```
   
   **ì›ë¦¬:**
   - "king" ë²¡í„°ì—ëŠ” "ì™•"ì´ë¼ëŠ” ê°œë…ê³¼ "ë‚¨ì„±"ì´ë¼ëŠ” ì†ì„± í¬í•¨
   - "man" ë¹¼ê¸° â†’ ë‚¨ì„± ì†ì„± ì œê±°
   - "woman" ë”í•˜ê¸° â†’ ì—¬ì„± ì†ì„± ì¶”ê°€
   - ê²°ê³¼ â†’ "queen" (ì—¬ì™•)ì˜ ì˜ë¯¸

4. **ì‹¤ìŠµ ë‹¨ê³„**
   - Sentence-Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ë¡œë“œ
   - ë‹¨ì–´ ì„ë² ë”© ìƒì„±
   - ìœ ì‚¬ë„ ê³„ì‚°: (king, queen), (king, man), (king, woman)
   - ì˜ë¯¸ ìœ ì¶” í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
   - ê²°ê³¼ ë¶„ì„

**ğŸ’¾ íŒŒì¼ëª…:** `llm_1_3_word_embeddings.py`


**ğŸ” ê¸°ëŒ€ ì¶œë ¥:**
```
=== ë¬¸ì œ 1.3: ì‹œë§¨í‹± ëŠ¥ë ¥ì˜ ë°œí˜„ - ë‹¨ì–´ ì„ë² ë”© ë¹„êµ ===

--- 2. ì˜ë¯¸ì  ìœ ì‚¬ë„ ë¹„êµ ---
ìœ ì‚¬ë„ ('king', 'queen'): 0.8234 (ì™•-ì—¬ì™•: ë†’ì€ ìœ ì‚¬ë„)
ìœ ì‚¬ë„ ('king', 'man'):   0.7123 (ì™•-ë‚¨ì: ì¤‘ê°„ ìœ ì‚¬ë„)
ìœ ì‚¬ë„ ('king', 'woman'): 0.4532 (ì™•-ì—¬ì: ë‚®ì€ ìœ ì‚¬ë„)

--- 3. ë²¡í„° ì—°ì‚°ì„ í†µí•œ ë‹¨ì–´ ìœ ì¶” ---
ìˆ˜í–‰í•  ì—°ì‚°: king - man + woman â‰ˆ queen
ì—°ì‚° ê²°ê³¼ ë²¡í„°ì™€ 'queen' ë²¡í„°ì˜ ìœ ì‚¬ë„: 0.7891
```

**ğŸ’¡ í•µì‹¬ ê°œë…:**
- **ì˜ë¯¸ ê´€ê³„ í•™ìŠµ:** ì„ë² ë”©ì€ ë‹¨ì–´ ê°„ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ë²¡í„° ê³µê°„ êµ¬ì¡°ë¡œ í‘œí˜„
- **ë²¡í„° ì—°ì‚°ì˜ ì˜ë¯¸:** ë²¡í„° ë§ì…ˆ/ëº„ì…ˆì€ ì˜ë¯¸ì˜ ì¡°í•©/ì œê±° ì˜ë¯¸
- **ì „ì´í•™ìŠµ:** ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ í•™ìŠµëœ ì„ë² ë”©ì„ ë‹¤ì–‘í•œ ì‘ì—…ì— ì¬ì‚¬ìš©

---

## Chapter 2: ìì—°ì–´ ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ ê°œë…

### ë¬¸ì œ 2.1: LSTMì„ ì´ìš©í•œ ìˆœì°¨ ë°ì´í„° ëª¨ë¸ë§ ë° ê°ì„± ë¶„ì„

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
ìˆœì°¨ì  ë°ì´í„°ì˜ ì‹œê°„ì  ì˜ì¡´ì„±ì„ í•™ìŠµí•˜ëŠ” LSTM ì‹ ê²½ë§ì„ ì´í•´í•˜ê³ , ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ê°ì •(ê°ì„±)ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **LSTMì˜ í•„ìš”ì„±**
   
   **RNNì˜ í•œê³„:** ê¸´ ì‹œí€€ìŠ¤ì—ì„œ "ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)" ë¬¸ì œ ë°œìƒ
   - ì…ë ¥(ì²˜ìŒ) â†’ ì¶œë ¥(ë)ê¹Œì§€ ê°€ëŠ” ê²½ë¡œê°€ ë„ˆë¬´ ê¸¸ì–´ì„œ ì •ë³´ ì†ì‹¤
   
   **LSTMì˜ í•´ê²°ë²•:** íŠ¹ë³„í•œ ë©”ëª¨ë¦¬ ì…€(Cell)ë¡œ ì¤‘ìš” ì •ë³´ ë³´ì¡´
   - ì…ë ¥ ê²Œì´íŠ¸: ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì…€ì— ë”í• ì§€ ê²°ì •
   - ë§ê° ê²Œì´íŠ¸: ì´ì „ ì •ë³´ ì¤‘ ë²„ë¦´ ê²ƒ ê²°ì •
   - ì¶œë ¥ ê²Œì´íŠ¸: ì…€ì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ êº¼ë‚¼ì§€ ê²°ì •

2. **ëª¨ë¸ êµ¬ì¡°**
   ```
   ì…ë ¥ ë¬¸ì¥
      â†“
   [ì„ë² ë”© ë ˆì´ì–´]  â†’ ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ ì˜ë¯¸ ë²¡í„°ë¡œ ë³€í™˜
      â†“
   [LSTM ë ˆì´ì–´]   â†’ ë¬¸ì¥ì˜ ìˆœì°¨ì  ì˜ì¡´ì„± í•™ìŠµ (ë¬¸ë§¥ íŒŒì•…)
      â†“
   [ì™„ì „ ì—°ê²° ë ˆì´ì–´] â†’ ë¶„ë¥˜ ìˆ˜í–‰ (ê¸ì •/ë¶€ì •)
      â†“
   [ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜]  â†’ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜ (0~1)
      â†“
   ê²°ê³¼: ê¸ì •(1) ë˜ëŠ” ë¶€ì •(0)
   ```

3. **í›ˆë ¨ ë°ì´í„°**
   ```python
   {
       "This movie was fantastic and amazing": 1,           # ê¸ì •
       "The acting was terrible and boring": 0,             # ë¶€ì •
       "I really enjoyed the plot and characters": 1,       # ê¸ì •
       "A complete waste of time and money": 0,             # ë¶€ì •
       "The visuals were stunning, a masterpiece": 1,       # ê¸ì •
       "I would not recommend this film to anyone": 0       # ë¶€ì •
   }
   ```

4. **ì‹¤ìŠµ ë‹¨ê³„**
   - ë°ì´í„° ì¤€ë¹„: í…ìŠ¤íŠ¸ â†’ ì •ìˆ˜ ì¸ì½”ë”© â†’ íŒ¨ë”©
   - ëª¨ë¸ ì •ì˜: Embedding + LSTM + FC + Sigmoid
   - ëª¨ë¸ í›ˆë ¨: 100 ì—í¬í¬, ì´ì§„ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜
   - ëª¨ë¸ í…ŒìŠ¤íŠ¸: ìƒˆ ë¬¸ì¥ ê°ì„± ë¶„ë¥˜

5. **ê²°ê³¼ í•´ì„**
   - ì˜ˆì¸¡ ì ìˆ˜ > 0.5 â†’ ê¸ì •
   - ì˜ˆì¸¡ ì ìˆ˜ < 0.5 â†’ ë¶€ì •
   - ì ìˆ˜ê°€ 0.5ì— ê°€ê¹Œìš¸ìˆ˜ë¡ í™•ì‹ ë„ ë‚®ìŒ

**ğŸ’¾ íŒŒì¼ëª…:** `llm_2_1_lstm_sentiment.py`

**ğŸ” ê¸°ëŒ€ ì¶œë ¥:**
```
=== ë¬¸ì œ 2.1: LSTMì„ ì´ìš©í•œ ìˆœì°¨ ë°ì´í„° ëª¨ë¸ë§ ===

ì–´íœ˜ ì‚¬ì „ í¬ê¸°: 22
ë°ì´í„°ê°€ (6, 12) ëª¨ì–‘ì˜ í…ì„œë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.

--- 2. ëª¨ë¸ í›ˆë ¨ ---
Epoch 20/100, Loss: 0.6234
Epoch 40/100, Loss: 0.4567
Epoch 60/100, Loss: 0.2891
Epoch 80/100, Loss: 0.1234
Epoch 100/100, Loss: 0.0567

--- 3. ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---
í…ŒìŠ¤íŠ¸ ë¬¸ì¥: 'The movie was good and enjoyable'
ì˜ˆì¸¡ ì ìˆ˜: 0.8234
ì˜ˆì¸¡ëœ ê°ì„±: ê¸ì •

í…ŒìŠ¤íŠ¸ ë¬¸ì¥: 'The plot was predictable and dull'
ì˜ˆì¸¡ ì ìˆ˜: 0.2145
ì˜ˆì¸¡ëœ ê°ì„±: ë¶€ì •
```

**ğŸ’¡ í•µì‹¬ ê°œë…:**
- **ì„ë² ë”©:** ë‹¨ì–´ë¥¼ ì˜ë¯¸ ë²¡í„°ë¡œ í‘œí˜„ (ê°™ì€ ì˜ë¯¸ì˜ ë‹¨ì–´ â†’ ìœ ì‚¬í•œ ë²¡í„°)
- **LSTM:** ìˆœì°¨ ë°ì´í„°ì˜ "ë¬¸ë§¥"ì„ í•™ìŠµ (ë‹¨ì–´ ìˆœì„œ ì¤‘ìš”)
- **ë¶„ë¥˜:** ìµœì¢… ì€ë‹‰ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°ì„± ë¶„ë¥˜

---

### ë¬¸ì œ 2.2: BERTë¥¼ í™œìš©í•œ ì „ì´í•™ìŠµì˜ ìœ„ë ¥

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬, ì ì€ ì½”ë“œì™€ ì ì€ ë°ì´í„°ë¡œ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ì „ì´í•™ìŠµ(Transfer Learning)ì˜ ê°•ë ¥í•¨ì„ ê²½í—˜í•©ë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **ì „ì´í•™ìŠµì˜ ê°œë…**
   
   **ê¸°ì¡´ ë°©ì‹ (ë¬¸ì œ 2.1):**
   ```
   ì‘ì€ í›ˆë ¨ ë°ì´í„° â†’ ëª¨ë¸ ì •ì˜ â†’ ë§¤ê°œë³€ìˆ˜ ì´ˆê¸°í™” (ë¬´ì‘ìœ„)
   â†’ ì²˜ìŒë¶€í„° í•™ìŠµ â†’ ì„±ëŠ¥ ë‚®ìŒ
   ```
   
   **ì „ì´í•™ìŠµ ë°©ì‹:**
   ```
   ëŒ€ê·œëª¨ ì™¸ë¶€ ë°ì´í„° (ì˜ˆ: ìœ„í‚¤í”¼ë””ì•„)
              â†“
   BERT ì‚¬ì „ í›ˆë ¨ (ì´ë¯¸ ì™„ë£Œë¨)
              â†“
   ë§¤ê°œë³€ìˆ˜ ì‚¬ìš© (ì´ë¯¸ ì¢‹ì€ íŠ¹ì„± í•™ìŠµë¨)
              â†“
   ìš°ë¦¬ì˜ ì‘ì€ ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •(Fine-tuning)
              â†“
   ë†’ì€ ì„±ëŠ¥!
   ```

2. **BERTì˜ íŠ¹ì§•**
   - **ì–‘ë°©í–¥:** ë¬¸ë§¥ì„ ì–‘ìª½ ëª¨ë‘ì—ì„œ ë´„
   - **ì‚¬ì „ í›ˆë ¨:** ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§ + ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡
   - **ì¼ë°˜í™”:** ë‹¤ì–‘í•œ NLP ì‘ì—…ì— ì ìš© ê°€ëŠ¥

3. **Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬**
   - `transformers`: ë‹¤ì–‘í•œ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ì œê³µ
   - `pipeline`: ë³µì¡í•œ ì½”ë“œë¥¼ ê°„ë‹¨í•˜ê²Œ ì¶”ìƒí™”
   
   **ì‚¬ìš©ë²•:**
   ```python
   from transformers import pipeline
   classifier = pipeline("sentiment-analysis")
   result = classifier("I love this movie!")
   # ê²°ê³¼: [{'label': 'POSITIVE', 'score': 0.9998}]
   ```

4. **ë¹„êµ ë¶„ì„**
   
   | í•­ëª© | LSTM (ì§ì ‘ êµ¬ì¶•) | BERT (ì „ì´í•™ìŠµ) |
   |------|-----------------|-----------------|
   | ì½”ë“œ ê¸¸ì´ | ~200ì¤„ | ~10ì¤„ |
   | í›ˆë ¨ ë°ì´í„° | 6ê°œ (ì‘ìŒ) | í•„ìš” ì—†ìŒ (ë¯¸ë¦¬ í•™ìŠµë¨) |
   | ì„±ëŠ¥ | ì¤‘ê°„ | ìš°ìˆ˜ |
   | í›ˆë ¨ ì‹œê°„ | ëª‡ ì´ˆ | ì—†ìŒ (ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ë¡œë“œ) |

5. **ë‹¤ì–‘í•œ ì…ë ¥ í…ŒìŠ¤íŠ¸**
   - ëª…í™•í•œ ê¸ì •/ë¶€ì • ë¬¸ì¥
   - ì¤‘ë¦½ì ì¸ ë¬¸ì¥
   - ë³µí•© ê°ì • ë¬¸ì¥
   - ë¹„ê°ì • í…ìŠ¤íŠ¸

**ğŸ’¾ íŒŒì¼ëª…:** `llm_2_2_bert_sentiment.py`

**ğŸ” ê¸°ëŒ€ ì¶œë ¥:**
```
=== ë¬¸ì œ 2.2: BERTë¥¼ í™œìš©í•œ ì „ì´í•™ìŠµì˜ ìœ„ë ¥ ===

í…ŒìŠ¤íŠ¸ ë¬¸ì¥: "I am absolutely thrilled with the results!"
ì˜ˆì¸¡ ê²°ê³¼: POSITIVE (ì‹ ë¢°ë„: 0.9998)

í…ŒìŠ¤íŠ¸ ë¬¸ì¥: "This is the worst experience I have ever had."
ì˜ˆì¸¡ ê²°ê³¼: NEGATIVE (ì‹ ë¢°ë„: 0.9995)

í…ŒìŠ¤íŠ¸ ë¬¸ì¥: "The movie was okay, but I probably wouldn't watch it again."
ì˜ˆì¸¡ ê²°ê³¼: NEGATIVE (ì‹ ë¢°ë„: 0.8234)
```

**ğŸ’¡ í•µì‹¬ ê°œë…:**
- **ì „ì´í•™ìŠµ:** í•œ ì‘ì—…ì—ì„œ í•™ìŠµí•œ ì§€ì‹ì„ ë‹¤ë¥¸ ì‘ì—…ì— ì ìš©
- **ë¯¸ì„¸ ì¡°ì •:** ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ì„ ìš°ë¦¬ ë°ì´í„°ì— ë§ê²Œ ì¡°ì •
- **íš¨ìœ¨ì„±:** ëŒ€ê·œëª¨ ë°ì´í„°ì™€ ì»´í“¨íŒ… ì—†ì´ë„ ê³ ì„±ëŠ¥ ë‹¬ì„± ê°€ëŠ¥

---

## Chapter 3: ì´ˆê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ ì‹¤ì œì  í™œìš©

### ë¬¸ì œ 3.1: GPTë¥¼ ì´ìš©í•œ í…ìŠ¤íŠ¸ ì™„ì„±

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
ìë™ íšŒê·€(Autoregressive) ëª¨ë¸ì¸ GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ìƒì„± íŒŒë¼ë¯¸í„°ê°€ ì¶œë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì´í•´í•©ë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **ìƒì„± ëª¨ë¸ì˜ ì‘ë™ ì›ë¦¬**
   ```
   í”„ë¡¬í”„íŠ¸: "Once upon a time"
   
   ë‹¨ê³„ 1: ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ â†’ "there" (í™•ë¥ : 0.8)
   í˜„ì¬: "Once upon a time there"
   
   ë‹¨ê³„ 2: ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ â†’ "was" (í™•ë¥ : 0.7)
   í˜„ì¬: "Once upon a time there was"
   
   ë‹¨ê³„ 3: ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ â†’ "a" (í™•ë¥ : 0.6)
   í˜„ì¬: "Once upon a time there was a"
   
   ... (ë°˜ë³µ)
   ```

2. **ìƒì„± íŒŒë¼ë¯¸í„°**
   
   - **temperature**: ì°½ì˜ì„± ì œì–´ (0 ~ 1+)
     - ë‚®ìŒ (0.1): ê²°ì •ì , ì˜ˆì¸¡ ê°€ëŠ¥ (ê°™ì€ ê²°ê³¼ ë°˜ë³µ)
     - ì¤‘ê°„ (0.7): ê· í˜•ì¡íŒ ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±
     - ë†’ìŒ (1.5): ë§¤ìš° ì°½ì˜ì , ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥
   
   - **top_p**: ëˆ„ì  í™•ë¥  ê¸°ë°˜ ìƒ˜í”Œë§
     - 0.9: ìƒìœ„ 90% í™•ë¥ ì˜ ë‹¨ì–´ë“¤ë§Œ ì„ íƒ
     - 0.5: ìƒìœ„ 50% í™•ë¥ ì˜ ë‹¨ì–´ë“¤ë§Œ ì„ íƒ (ë” ì§‘ì¤‘)
   
   - **max_length**: ìƒì„± ìµœëŒ€ ê¸¸ì´ (50~100 ë‹¨ì–´)

3. **ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸**
   - "The future of AI is..." (ê¸°ìˆ )
   - "Once upon a time..." (ì´ì•¼ê¸°)
   - "The most important lesson..." (êµìœ¡)

4. **ìƒì„± ê²°ê³¼ ë¶„ì„**
   - ê° ì˜¨ë„ ì„¤ì •ë³„ ê²°ê³¼ ë¹„êµ
   - ì°½ì˜ì„± vs ì¼ê´€ì„± íŠ¸ë ˆì´ë“œì˜¤í”„
   - ì‹¤ë¬´ í™œìš©: ê³ ê° ì„œë¹„ìŠ¤(ì¼ê´€ì„± ì¤‘ìš”) vs ì°½ì‘(ì°½ì˜ì„± ì¤‘ìš”)

**ğŸ’¾ íŒŒì¼ëª…:** `llm_3_1_gpt_text_completion.py`



---

### ë¬¸ì œ 3.2: ì°½ì˜ì„± ì œì–´í•˜ê¸° - ìƒì„± íŒŒë¼ë¯¸í„°ì˜ ì˜í–¥

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
ìƒì„± ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë“¤ì´ ì¶œë ¥ì— ë¯¸ì¹˜ëŠ” êµ¬ì²´ì ì¸ ì˜í–¥ì„ ì‹¤í—˜ì„ í†µí•´ ì´í•´í•©ë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **íŒŒë¼ë¯¸í„°ë³„ ìƒì„± ë¹„êµ**
   - ë™ì¼ í”„ë¡¬í”„íŠ¸
   - ë‹¤ì–‘í•œ temperature (0.3, 0.7, 1.0, 1.3)
   - ë‹¤ì–‘í•œ top_p (0.9, 0.7, 0.5)

2. **ì •ëŸ‰ì  ë¶„ì„**
   - ë¬¸ì¥ ê¸¸ì´ ë¹„êµ
   - ê³ ìœ  ë‹¨ì–´ ê°œìˆ˜
   - ë°˜ë³µ íŒ¨í„´ ë¶„ì„

3. **ì •ì„±ì  ë¶„ì„**
   - ê²°ê³¼ì˜ ì¼ê´€ì„±
   - ì°½ì˜ì„± ìˆ˜ì¤€
   - í˜„ì‹¤ì„±/íƒ€ë‹¹ì„±

**ğŸ’¾ íŒŒì¼ëª…:** `llm_3_2_generation_parameters.py`

---

### ë¬¸ì œ 3.3: ê°„ë‹¨í•œ ë„ë©”ì¸ ì‘ìš© - ë§ˆì¼€íŒ… ì¹´í”¼ ìƒì„±ê¸°

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œ(ë§ˆì¼€íŒ… ì½˜í…ì¸  ìƒì„±)ì— LLMì„ ì ìš©í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **ë§ˆì¼€íŒ… ì¹´í”¼ì˜ íŠ¹ì„±**
   - ê°„ê²°ì„±: ì§§ê³  ì„íŒ©íŠ¸ ìˆìŒ
   - í–‰ë™ ìœ ë„: í´ë¦­/êµ¬ë§¤ ìœ ë„
   - ê°ì • í˜¸ì†Œ: ê¸ì •ì  ê°ì • ìê·¹

2. **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**
   ```python
   í”„ë¡¬í”„íŠ¸ = """
   ë‹¤ìŒì€ ì œí’ˆ ì„¤ëª…ì…ë‹ˆë‹¤:
   ì œí’ˆëª…: ìŠ¤ë§ˆíŠ¸ ì›Œí„°ë³‘
   íŠ¹ì§•: ìˆ˜ë¶„ ì„­ì·¨ ì¶”ì , ì•± ì—°ë™, ìë™ ì•Œë¦¼
   
   ì´ ì œí’ˆì„ ìœ„í•œ 3ì¤„ì§œë¦¬ ë§ˆì¼€íŒ… ì¹´í”¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
   ì¹´í”¼ëŠ” ì Šì€ ì‚¬ëŒë“¤ì„ íƒ€ê²Ÿìœ¼ë¡œ í•˜ë©°, í–‰ë™ì„ ìœ ë„í•˜ëŠ” ë‚´ìš©ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
   """
   ```

3. **ë‹¤ì–‘í•œ ì œí’ˆ ì¹´í”¼ ìƒì„±**
   - ìµœì‹  ìŠ¤ë§ˆíŠ¸í°
   - ì˜¤ê°€ë‹‰ ì»¤í”¼
   - í”¼íŠ¸ë‹ˆìŠ¤ ì•±

**ğŸ’¾ íŒŒì¼ëª…:** `llm_3_3_marketing_copy_generator.py`

---

## Chapter 4: ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²•

### ë¬¸ì œ 4.1: í“¨ìƒ·(Few-Shot) ì¸-ì»¨í…ìŠ¤íŠ¸ ëŸ¬ë‹

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
í”„ë¡¬í”„íŠ¸ì— ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•ì„ ë°°ì›ë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **ì œë¡œìƒ· vs í“¨ìƒ·**
   
   **ì œë¡œìƒ· (ì˜ˆì‹œ ì—†ìŒ):**
   ```
   ì…ë ¥: "The Eiffel Tower is in France"
   ì‘ì—…: ì´ ë¬¸ì¥ì˜ ê°ì„±ì„ ë¶„ë¥˜í•˜ì„¸ìš”.
   ëª¨ë¸ì´ ê°ì„±ì´ ì—†ì–´ì„œ ì œëŒ€ë¡œ ë¶„ë¥˜í•˜ì§€ ëª»í•¨
   ```
   
   **í“¨ìƒ· (ì˜ˆì‹œ í¬í•¨):**
   ```
   ì˜ˆì‹œ 1:
   ë¬¸ì¥: "I love this movie"
   ê°ì„±: Positive
   
   ì˜ˆì‹œ 2:
   ë¬¸ì¥: "This is terrible"
   ê°ì„±: Negative
   
   ì´ì œ ë‹¤ìŒì„ ë¶„ë¥˜í•˜ì„¸ìš”:
   ë¬¸ì¥: "The Eiffel Tower is beautiful"
   ê°ì„±: ?
   ```

2. **í“¨ìƒ· í”„ë¡¬í”„íŠ¸ ì‘ì„±**
   - 2~3ê°œì˜ ëŒ€í‘œì ì¸ ì˜ˆì‹œ
   - ëª…í™•í•œ ì…ì¶œë ¥ í˜•ì‹
   - ë‹¤ì–‘í•œ ë²”ì£¼ í¬í•¨

3. **ì„±ëŠ¥ ë¹„êµ**
   - ì œë¡œìƒ· ê²°ê³¼
   - í“¨ìƒ· ê²°ê³¼ (2-shot, 3-shot)
   - ì„±ëŠ¥ í–¥ìƒë„ ì¸¡ì •

**ğŸ’¾ íŒŒì¼ëª…:** `llm_4_1_few_shot_learning.py`

---

### ë¬¸ì œ 4.2: LangChainì„ ì´ìš©í•œ ê¸°ë³¸ RAG ì‹œìŠ¤í…œ êµ¬ì¶•

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
ê²€ìƒ‰-ì¦ê°• ìƒì„±(RAG: Retrieval-Augmented Generation)ì˜ ê°œë…ê³¼ êµ¬í˜„ì„ ë°°ì›ë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **RAGì˜ í•„ìš”ì„±**
   - LLMì˜ í•œê³„: í•™ìŠµ ë°ì´í„°ì—ë§Œ ê¸°ë°˜ (ì‹œê°„ì´ ì§€ë‚˜ë©´ ì˜¤ë˜ëœ ì •ë³´)
   - í•´ê²°ì±…: ìµœì‹  ì •ë³´ë¥¼ ë™ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ì œê³µ

2. **RAG íŒŒì´í”„ë¼ì¸**
   ```
   ì‚¬ìš©ì ì§ˆë¬¸
        â†“
   [ë¬¸ì„œ ê²€ìƒ‰] â†’ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° (ìœ ì‚¬ë„ ê¸°ë°˜)
        â†“
   [ì •ë³´ ì¶”ê°€] â†’ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        â†“
   [í…ìŠ¤íŠ¸ ìƒì„±] â†’ LLMì´ ì œê³µëœ ì •ë³´ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
        â†“
   ìµœì¢… ë‹µë³€
   ```

3. **LangChain ì‚¬ìš©**
   - ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
   - ìœ ì‚¬ë„ ê²€ìƒ‰
   - ì²´ì¸ êµ¬ì„±

**ğŸ’¾ íŒŒì¼ëª…:** `llm_4_2_langchain_rag.py`

---

### ë¬¸ì œ 4.3: ì‚¬ê³ ì˜ ì—°ì‡„(Chain-of-Thought)ë¥¼ í†µí•œ ì¶”ë¡  ìœ ë„

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
ëª¨ë¸ì´ ë‹¨ê³„ë³„ë¡œ ì¶”ë¡ í•˜ë„ë¡ ìœ ë„í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **Chain-of-Thoughtì˜ ì›ë¦¬**
   
   **ì¼ë°˜ì  í”„ë¡¬í”„íŠ¸:**
   ```
   ì§ˆë¬¸: "13 Ã— 7 - 15 = ?"
   ë‹µ: ?
   ```
   í° ëª¨ë¸ë„ ê³„ì‚° ì‹¤ìˆ˜ ê°€ëŠ¥
   
   **Chain-of-Thought í”„ë¡¬í”„íŠ¸:**
   ```
   ë¬¸ì œ: 13 Ã— 7 - 15 = ?
   ë‹¨ê³„ë³„ë¡œ í’€ì–´ë³´ì„¸ìš”:
   
   ë‹¨ê³„ 1: 13 Ã— 7 = ?
   ë‹¨ê³„ 2: ê²°ê³¼ì—ì„œ 15ë¥¼ ëº€ë‹¤
   ë‹¨ê³„ 3: ìµœì¢… ë‹µ
   ```

2. **ë³µì¡í•œ ì¶”ë¡  ë¬¸ì œ**
   - ìˆ˜í•™ ë¬¸ì œ
   - ë…¼ë¦¬ ë¬¸ì œ
   - ìƒì‹ ì¶”ë¡ 

**ğŸ’¾ íŒŒì¼ëª…:** `llm_4_3_chain_of_thought.py`

---

## Chapter 5: íŒŒì¸íŠœë‹ì„ í†µí•œ LLMì˜ íš¨ìœ¨ì  ì ì‘

### ë¬¸ì œ 5.1: PEFT(LoRA)ë¥¼ ì´ìš©í•œ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  íŒŒì¸íŠœë‹

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
ì „ì²´ ëª¨ë¸ì˜ ìˆ˜ì‹­ì–µ ê°œ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë‘ í›ˆë ¨í•˜ëŠ” ëŒ€ì‹ , ì†Œìˆ˜ì˜ ì¶”ê°€ íŒŒë¼ë¯¸í„°ë§Œ ì¡°ì •í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ì„ íŠ¹í™”ì‹œí‚¤ëŠ” ê¸°ë²•ì„ ë°°ì›ë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **LoRAì˜ ê°œë… ì´í•´**
   - ê¸°ì¡´ ê°€ì¤‘ì¹˜ WëŠ” ê³ ì •
   - A, B ë‘ ê°œì˜ ì‘ì€ í–‰ë ¬ë§Œ ì¶”ê°€ (Low-Rank: ì‘ì€ í¬ê¸°)
   - í›ˆë ¨ ì‹œ A, Bë§Œ ì—…ë°ì´íŠ¸
   - ê³µì‹: $h = W_0x + BA x$ (BÃ—AëŠ” LoRA ì–´ëŒ‘í„°)

2. **LoRA ì–´ëŒ‘í„° ì„¤ì • - LoraConfig ê°ì²´ ìƒì„±**
   ```python
   from peft import LoraConfig, get_peft_model
   
   lora_config = LoraConfig(
       r=8,                                    # ë­í¬ (ì ì„ìˆ˜ë¡ ê²½ëŸ‰, í´ìˆ˜ë¡ í‘œí˜„ë ¥ ì¦ê°€)
       lora_alpha=16,                          # ìŠ¤ì¼€ì¼ë§ íŒ©í„°
       target_modules=["c_attn", "c_proj"],   # ì–´ë–¤ ê°€ì¤‘ì¹˜ì— LoRA ì ìš©í• ì§€
       lora_dropout=0.1,                      # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
       bias="none",                           # bias í›ˆë ¨ ì—¬ë¶€
       task_type="CAUSAL_LM"                  # ì‘ì—… íƒ€ì…
   )
   ```

3. **LoRA ëª¨ë¸ ìƒì„± ë° íŒŒë¼ë¯¸í„° í™•ì¸**
   ```python
   # distilgpt2 ëª¨ë¸ ë¡œë“œ
   from transformers import AutoModelForCausalLM
   base_model = AutoModelForCausalLM.from_pretrained("distilgpt2")
   
   # LoRA ì–´ëŒ‘í„° ì ìš©
   peft_model = get_peft_model(base_model, lora_config)
   peft_model.print_trainable_parameters()
   
   # ì¶œë ¥: trainable params: 245,760 || all params: 82,112,000 || trainable%: 0.2993
   ```

4. **í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ (ë²•ë¥  ë°ì´í„°)**
   ```python
   legal_data = [
       {
           "instruction": "What is a tort?",
           "output": "A tort is a civil wrong that causes a claimant to suffer loss or harm, resulting in legal liability."
       },
       {
           "instruction": "Explain the concept of 'habeas corpus'.",
           "output": "Habeas corpus is a legal recourse through which a person can report an unlawful detention or imprisonment to a court."
       },
       {
           "instruction": "What does 'pro bono' mean?",
           "output": "'Pro bono publico', often shortened to 'pro bono', is professional work undertaken voluntarily and without payment."
       }
   ]
   ```

5. **í›ˆë ¨ ë°ì´í„° í¬ë§·íŒ…**
   - ê° ì˜ˆì œë¥¼ "### Instruction:\n{instruction}\n\n### Response:\n{output}" í˜•ì‹ìœ¼ë¡œ ë³€í™˜
   - Dataset ê°ì²´ë¡œ ë³€í™˜

6. **í›ˆë ¨ ì„¤ì • (TrainingArguments)**
   ```python
   training_args = TrainingArguments(
       output_dir="./lora_results",
       per_device_train_batch_size=1,
       num_train_epochs=10,
       logging_steps=1,
       learning_rate=2e-4
   )
   ```

7. **SFTTrainerë¡œ í›ˆë ¨ ì‹¤í–‰**
   ```python
   trainer = SFTTrainer(
       model=peft_model,
       train_dataset=dataset,
       dataset_text_field="text",
       args=training_args,
       max_seq_length=128
   )
   trainer.train()  # í›ˆë ¨ ì‹œì‘
   ```

8. **íŒŒì¸íŠœë‹ ì „í›„ ë¹„êµ í…ŒìŠ¤íŠ¸**
   ```python
   prompt = "### Instruction:\nWhat is a tort?\n\n### Response:\n"
   
   # ê¸°ë³¸ ëª¨ë¸ ì‘ë‹µ ìƒì„±
   inputs = tokenizer(prompt, return_tensors="pt")
   base_outputs = base_model.generate(**inputs, max_new_tokens=60)
   print("ê¸°ë³¸ ëª¨ë¸:", tokenizer.decode(base_outputs[0]))
   
   # LoRA íŒŒì¸íŠœë‹ ëª¨ë¸ ì‘ë‹µ ìƒì„±
   lora_outputs = peft_model.generate(**inputs, max_new_tokens=60)
   print("LoRA ëª¨ë¸:", tokenizer.decode(lora_outputs[0]))
   ```

9. **íš¨ìœ¨ì„± ë¹„êµ**
   - ì „ì²´ í›ˆë ¨ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
   - LoRA í›ˆë ¨ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
   - ì €ì¥ íŒŒì¼ í¬ê¸°: ì „ì²´ ëª¨ë¸ vs ì–´ëŒ‘í„°ë§Œ

**í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬:**
```bash
pip install transformers datasets peft accelerate trl
```

**ğŸ’¾ íŒŒì¼ëª…:** `llm_5_1_lora_finetuning.py`


**ğŸ” ê¸°ëŒ€ ì¶œë ¥:**
```
=== ë¬¸ì œ 5.1: PEFT(LoRA)ë¥¼ ì´ìš©í•œ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  íŒŒì¸íŠœë‹ ===
trainable params: 245,760 || all params: 82,112,000 || trainable%: 0.2993

LoRA í›ˆë ¨ ì‹œì‘...
Epoch 1/10: Loss = 3.245
Epoch 2/10: Loss = 2.156
...
LoRA í›ˆë ¨ ì™„ë£Œ.

--- ê¸°ë³¸ ëª¨ë¸ ì‘ë‹µ ---
### Instruction:
What is a tort?

### Response:
[ê¸°ë³¸ ëª¨ë¸ì˜ ì¼ë°˜ì ì¸ ì‘ë‹µ]

--- LoRA íŒŒì¸íŠœë‹ ëª¨ë¸ ì‘ë‹µ ---
### Instruction:
What is a tort?

### Response:
A tort is a civil wrong that causes a claimant to suffer loss or harm, resulting in legal liability for the person who commits the tortious act.
```

---

### ë¬¸ì œ 5.2: ê°œë…ì  RLHF - ë³´ìƒ ëª¨ë¸ì˜ ì—­í•  ì´í•´

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
ì¸ê°„ì´ ì–´ë–¤ ì‘ë‹µì´ ë” ì¢‹ì€ì§€ í‰ê°€í•˜ëŠ” ì„ í˜¸ë„ë¥¼ í•™ìŠµí•˜ì—¬, LLMì˜ ì¶œë ¥ì„ ê°œì„ í•˜ëŠ” ë³´ìƒ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **ë³´ìƒ ëª¨ë¸ì˜ ëª©ì **
   - ë‘ ê°œì˜ ì‘ë‹µ ì¤‘ ì–´ëŠ ê²ƒì´ ë” ë‚˜ì€ì§€ íŒë‹¨
   - ì ìˆ˜ë¡œ í‘œí˜„: ë” ì¢‹ì€ ì‘ë‹µì—ëŠ” ë†’ì€ ì ìˆ˜, ë‚˜ìœ ì‘ë‹µì—ëŠ” ë‚®ì€ ì ìˆ˜
   - ì´í›„ ê°•í™”í•™ìŠµì—ì„œ LLM í›ˆë ¨ì— ì‚¬ìš©

2. **íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ì ìˆ˜ í•¨ìˆ˜ êµ¬í˜„**
   
   **ì ìˆ˜ ê³„ì‚° ê·œì¹™:**
   ```python
   def calculate_reward(prompt, response):
       score = 0
       
       # ê·œì¹™ 1: ì‘ë‹µ ê¸¸ì´ (ìµœì†Œ 50ì ì´ìƒ, 1000ì ì´í•˜ê°€ ìµœì )
       if 50 <= len(response) <= 1000:
           score += 2
       elif len(response) < 50:
           score -= 3  # ë„ˆë¬´ ì§§ìœ¼ë©´ ê°ì 
       
       # ê·œì¹™ 2: íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ (ë„ì›€ì´ ë˜ëŠ” í‘œí˜„)
       helpful_keywords = ["explain", "reason", "example", "detail", "because"]
       keyword_count = sum(1 for kw in helpful_keywords if kw in response.lower())
       score += keyword_count * 0.5
       
       # ê·œì¹™ 3: ë¶€ì •ì  í‘œí˜„ (í”¼í•´ì•¼ í•  í‘œí˜„)
       negative_keywords = ["sorry", "cannot", "don't know", "not sure"]
       negative_count = sum(1 for neg in negative_keywords if neg in response.lower())
       score -= negative_count * 1.0
       
       # ê·œì¹™ 4: ë¬¸ë²• (ë¬¸ì¥ì´ ë§ˆì¹¨í‘œë¡œ ëë‚˜ëŠ”ê°€)
       if response.strip().endswith('.'):
           score += 1
       
       return max(0, min(score, 10))  # 0~10 ë²”ìœ„ë¡œ ì •ê·œí™”
   ```

3. **í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ êµ¬ì„±**
   ```python
   scenarios = [
       {
           "prompt": "What is photosynthesis?",
           "response_a": "It's how plants eat.",
           "response_b": "Photosynthesis is the process by which plants harness sunlight and convert it into chemical energy."
       },
       {
           "prompt": "Explain climate change.",
           "response_a": "Climate change is bad.",
           "response_b": "Climate change refers to long-term shifts in global temperatures and weather patterns, primarily caused by human activities like burning fossil fuels."
       }
   ]
   ```

4. **ê° ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ ì ìˆ˜ ê³„ì‚°**
   ```python
   for scenario in scenarios:
       score_a = calculate_reward(scenario["prompt"], scenario["response_a"])
       score_b = calculate_reward(scenario["prompt"], scenario["response_b"])
       
       print(f"í”„ë¡¬í”„íŠ¸: {scenario['prompt']}")
       print(f"ì‘ë‹µ A ì ìˆ˜: {score_a}")
       print(f"ì‘ë‹µ B ì ìˆ˜: {score_b}")
       print(f"ì„ í˜¸ ì‘ë‹µ: {'B' if score_b > score_a else 'A'}")
   ```

5. **ë³´ìƒ ëª¨ë¸ì˜ í•™ìŠµ íš¨ê³¼ ë¶„ì„**
   - ê° ì‘ë‹µì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ ì ìˆ˜í™”
   - ì ìˆ˜ ì°¨ì´ ë¶„ì„
   - ì–´ë–¤ íŠ¹ì„±ì´ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ëŠ”ì§€ íŒ¨í„´ ì¸ì‹

**í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬:**
ì—†ìŒ (ê¸°ë³¸ Pythonë§Œ ì‚¬ìš©)

**ğŸ’¾ íŒŒì¼ëª…:** `llm_5_2_rlhf_reward_model.py`


**ğŸ” ê¸°ëŒ€ ì¶œë ¥:**
```
=== ë¬¸ì œ 5.2: ê°œë…ì  RLHF - ë³´ìƒ ëª¨ë¸ì˜ ì—­í•  ì´í•´ ===

=== ì‹œë‚˜ë¦¬ì˜¤ 1 ===
í”„ë¡¬í”„íŠ¸: What is photosynthesis?

ì‘ë‹µ A: "It's how plants eat."
ì ìˆ˜: 2/10
ê·¼ê±°:
  - ë„ˆë¬´ ì§§ìŒ (-3ì )
  - ë§ˆì¹¨í‘œë¡œ ëë‚¨ (+1ì )
  - ì„¤ëª… í‚¤ì›Œë“œ ë¶€ì¬

ì‘ë‹µ B: "Photosynthesis is the process by which plants harness sunlight..."
ì ìˆ˜: 9/10
ê·¼ê±°:
  - ì ì ˆí•œ ê¸¸ì´ (+2ì )
  - "process", "harness" ë“± í‚¤ì›Œë“œ í¬í•¨ (+2ì )
  - ë§ˆì¹¨í‘œë¡œ ëë‚¨ (+1ì )

ì„ í˜¸ë„: ì‘ë‹µ B âœ“

=== ê²°ë¡  ===
ë³´ìƒ ëª¨ë¸ì˜ ì—­í• :
1. ì‘ë‹µì˜ í’ˆì§ˆì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€
2. ë” ë‚˜ì€ ì‘ë‹µê³¼ ë‚˜ìœ ì‘ë‹µì„ êµ¬ë³„
3. ê°•í™”í•™ìŠµì—ì„œ LLMì„ ì¢‹ì€ ì‘ë‹µìœ¼ë¡œ ìœ ë„
```

---

## Chapter 6: ë©€í‹°ëª¨ë‹¬ AIì˜ ìµœì „ì„ 

### ë¬¸ì œ 6.1: ì´ë¯¸ì§€ ìº¡ì…”ë‹

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
Vision-Language ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ ìë™ìœ¼ë¡œ ì„¤ëª…í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ëŠ¥ë ¥ì„ ë°°ì›ë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **Vision-Language ëª¨ë¸ ì„ íƒ ë° ë¡œë“œ**
   ```python
   from transformers import BlipProcessor, BlipForConditionalGeneration
   
   processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
   model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
   ```

2. **ìƒ˜í”Œ ì´ë¯¸ì§€ ì†ŒìŠ¤**
   - PIL(Python Imaging Library)ë¡œ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±, ë˜ëŠ”
   - URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
   - ë¡œì»¬ íŒŒì¼ì—ì„œ ë¡œë“œ

3. **ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬**
   ```python
   from PIL import Image
   import requests
   
   # ë°©ë²• 1: URLì—ì„œ ë¡œë“œ
   url = "https://farm4.staticflickr.com/3693/11174302639_46e2961b12_z.jpg"
   image = Image.open(requests.get(url, stream=True).raw)
   
   # ë°©ë²• 2: ë¡œì»¬ íŒŒì¼
   image = Image.open("sample_image.jpg")
   ```

4. **ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± - ê¸°ë³¸ ëª¨ë“œ**
   ```python
   # í”„ë¡¬í”„íŠ¸ ì—†ì´ ìº¡ì…˜ ìƒì„±
   inputs = processor(image, return_tensors="pt")
   out = model.generate(**inputs)
   caption_unconditional = processor.decode(out[0], skip_special_tokens=True)
   ```

5. **ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± - ì¡°ê±´ë¶€ ëª¨ë“œ**
   ```python
   # íŠ¹ì • ì£¼ì œì— ë§ì¶˜ ìº¡ì…˜ ìƒì„±
   prompts = [
       "a dog",
       "an outdoor scene",
       "animals in their natural habitat"
   ]
   
   for prompt in prompts:
       inputs = processor(image, text=prompt, return_tensors="pt")
       out = model.generate(**inputs)
       caption = processor.decode(out[0], skip_special_tokens=True)
       print(f"í”„ë¡¬í”„íŠ¸: '{prompt}' â†’ ìº¡ì…˜: '{caption}'")
   ```

6. **ì—¬ëŸ¬ ì´ë¯¸ì§€ì— ëŒ€í•´ ìº¡ì…˜ ìƒì„± ë° ë¹„êµ**
   - ìµœì†Œ 3ê°œ ì´ìƒì˜ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì²˜ë¦¬
   - ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ì¡°ê±´ ì—†ì´, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¡œ ìº¡ì…˜ ìƒì„±
   - ìƒì„±ëœ ìº¡ì…˜ë“¤ì„ ë¹„êµ

7. **ê²°ê³¼ ì •ë¦¬ ë° ë¶„ì„**
   ```python
   results = {
       "image_path": "...",
       "unconditional_caption": "...",
       "conditional_captions": {
           "prompt1": "caption1",
           "prompt2": "caption2"
       }
   }
   ```

**í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬:**
```bash
pip install transformers torch pillow requests
```

**ğŸ’¾ íŒŒì¼ëª…:** `llm_6_1_image_captioning.py`


**ğŸ” ê¸°ëŒ€ ì¶œë ¥:**
```
=== ë¬¸ì œ 6.1: ì´ë¯¸ì§€ ìº¡ì…”ë‹ ===

ì´ë¯¸ì§€ 1: dogs_playing.jpg
ì¡°ê±´ ì—†ëŠ” ìº¡ì…˜: "two dogs playing in the grass"

ì¡°ê±´ë¶€ ìº¡ì…˜:
  - í”„ë¡¬í”„íŠ¸ "puppies" â†’ "two adorable puppies playing outdoors"
  - í”„ë¡¬í”„íŠ¸ "outdoor" â†’ "outdoor scene with animals"
  - í”„ë¡¬í”„íŠ¸ "sports" â†’ "dogs engaged in playful activity"

ì´ë¯¸ì§€ 2: mountain_landscape.jpg
ì¡°ê±´ ì—†ëŠ” ìº¡ì…˜: "a snow-covered mountain in the distance"

ì¡°ê±´ë¶€ ìº¡ì…˜:
  - í”„ë¡¬í”„íŠ¸ "nature" â†’ "beautiful natural landscape"
  - í”„ë¡¬í”„íŠ¸ "winter" â†’ "snowy mountain winter scene"

ë¶„ì„:
- ëª¨ë¸ì´ ì´ë¯¸ì§€ì˜ ì£¼ìš” ìš”ì†Œë¥¼ ì •í™•íˆ ì¸ì‹
- ì¡°ê±´ë¶€ í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ë‹¤ì–‘í•œ ê°ë„ì˜ ì„¤ëª… ìƒì„±
- Vision-Language ëª¨ë¸ì˜ ë‹¤ëª©ì  í™œìš© ëŠ¥ë ¥ í™•ì¸
```

---

### ë¬¸ì œ 6.2: í™•ì‚° ëª¨ë¸(Diffusion Model)ì„ ì´ìš©í•œ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±

**ğŸ’¡ í•™ìŠµ ëª©í‘œ:**
í…ìŠ¤íŠ¸ ì„¤ëª…(í”„ë¡¬í”„íŠ¸)ìœ¼ë¡œë¶€í„° ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ìƒì„±í˜• AI ê¸°ìˆ ì„ ë°°ì›ë‹ˆë‹¤.

**ğŸ“‹ ìƒì„¸ ì§€ì‹œì‚¬í•­:**

1. **Stable Diffusion ëª¨ë¸ ë¡œë“œ**
   ```python
   from diffusers import StableDiffusionPipeline
   import torch
   
   model_id = "runwayml/stable-diffusion-v1-5"
   pipe = StableDiffusionPipeline.from_pretrained(
       model_id, 
       torch_dtype=torch.float16
   )
   pipe = pipe.to("cpu")  # ë˜ëŠ” "cuda"
   ```

2. **ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±**
   ```python
   prompt = "a beautiful sunset over the ocean"
   image = pipe(prompt).images[0]
   image.save("output_1.png")
   ```

3. **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ - í’ˆì§ˆ í–¥ìƒ í‚¤ì›Œë“œ ì¶”ê°€**
   
   **í”„ë¡¬í”„íŠ¸ ê°œì„  ë‹¨ê³„:**
   ```
   ì›ë³¸: "a cat"
   â†“
   ê°œì„ : "a fluffy orange cat sitting on a wooden chair, 4K, highly detailed, professional photography"
   ```
   
   ```python
   prompts = [
       "a dog",  # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
       "a fluffy golden retriever, professional photography, 4K",  # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
   ]
   
   for prompt in prompts:
       image = pipe(prompt).images[0]
       image.save(f"output_{prompts.index(prompt)}.png")
   ```

4. **ìƒì„± íŒŒë¼ë¯¸í„° ì¡°ì •**
   ```python
   # íŒŒë¼ë¯¸í„°ë³„ ì˜í–¥ í…ŒìŠ¤íŠ¸
   
   # 1) num_inference_steps: ë” ë§ì„ìˆ˜ë¡ í’ˆì§ˆ ë†’ì§€ë§Œ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼
   image_fast = pipe(
       prompt, 
       num_inference_steps=20  # ë¹ ë¦„, í’ˆì§ˆ ë‚®ìŒ
   ).images[0]
   
   image_quality = pipe(
       prompt,
       num_inference_steps=50  # ëŠë¦¼, í’ˆì§ˆ ë†’ìŒ
   ).images[0]
   
   # 2) guidance_scale: í”„ë¡¬í”„íŠ¸ ì¶©ì‹¤ë„ (7.5ê°€ ê¸°ë³¸)
   image_less_guided = pipe(
       prompt,
       guidance_scale=3.0  # ë” ì°½ì˜ì , í”„ë¡¬í”„íŠ¸ì— ëœ ì¶©ì‹¤
   ).images[0]
   
   image_more_guided = pipe(
       prompt,
       guidance_scale=15.0  # ëœ ì°½ì˜ì , í”„ë¡¬í”„íŠ¸ì— ë” ì¶©ì‹¤
   ).images[0]
   
   # 3) seed: ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œê°’
   image_seed1 = pipe(
       prompt,
       generator=torch.Generator().manual_seed(42)
   ).images[0]
   ```

5. **ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì˜ ì´ë¯¸ì§€ ìƒì„±**
   ```python
   styles = [
       "a portrait of a person, oil painting style",
       "a landscape scene, anime style",
       "a futuristic city, digital art style",
       "a detailed map, watercolor style"
   ]
   
   for style in styles:
       image = pipe(style).images[0]
       image.save(f"style_{styles.index(style)}.png")
   ```

6. **Negative Promptë¥¼ ì´ìš©í•œ ì›ì¹˜ ì•ŠëŠ” íŠ¹ì„± ì œê±°**
   ```python
   positive_prompt = "a beautiful woman"
   negative_prompt = "blurry, low quality, deformed, bad anatomy"
   
   image = pipe(
       positive_prompt,
       negative_prompt=negative_prompt
   ).images[0]
   ```

7. **ì´ë¯¸ì§€ ìƒì„± ê²°ê³¼ ë¹„êµ ë° ë¶„ì„**
   - ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¡œ ìƒì„±í•œ ì´ë¯¸ì§€ë“¤ ë¹„êµ
   - ê° íŒŒë¼ë¯¸í„°ì˜ ì˜í–¥ ë¶„ì„
   - ìµœì ì˜ í”„ë¡¬í”„íŠ¸ì™€ íŒŒë¼ë¯¸í„° ì¡°í•© ì°¾ê¸°

**í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬:**
```bash
pip install diffusers torch transformers pillow
```

**ğŸ’¾ íŒŒì¼ëª…:** `llm_6_2_stable_diffusion.py`


**ğŸ” ê¸°ëŒ€ ì¶œë ¥:**
```
=== ë¬¸ì œ 6.2: Stable Diffusion í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± ===

ê¸°ë³¸ í”„ë¡¬í”„íŠ¸: "a beautiful sunset over the ocean"
ìƒì„± ì™„ë£Œ â†’ output_1.png

í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹¤í—˜:
ì›ë³¸: "a dog"
ê°œì„ : "a golden retriever running through a field, professional photography, 4K, highly detailed"
ìƒì„± ì™„ë£Œ â†’ output_2.png

íŒŒë¼ë¯¸í„° ë¹„êµ:
- num_inference_steps=20 (ë¹ ë¦„, 2ì´ˆ) â†’ output_fast.png
- num_inference_steps=50 (ëŠë¦¼, 8ì´ˆ) â†’ output_quality.png

Guidance Scale ë¹„êµ:
- guidance_scale=3.0 (ì°½ì˜ì ) â†’ output_creative.png
- guidance_scale=15.0 (ì¶©ì‹¤) â†’ output_faithful.png

ìŠ¤íƒ€ì¼ ë‹¤ì–‘í™”:
- Oil painting: "a landscape, oil painting" â†’ style_oil.png
- Anime: "a girl, anime style" â†’ style_anime.png
- Digital art: "futuristic city, digital art" â†’ style_digital.png

Negative Prompt íš¨ê³¼:
- í”„ë¡¬í”„íŠ¸: "a beautiful woman"
- Negative: "blurry, low quality, deformed"
- ê²°ê³¼: ì„ ëª…í•˜ê³  ê¹”ë”í•œ ì´ˆìƒí™” ìƒì„± â†’ output_improved.png

ë¶„ì„:
1. num_inference_stepsê°€ ë†’ì„ìˆ˜ë¡ ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ
2. guidance_scaleì€ ì°½ì˜ì„±ê³¼ ì •í™•ì„±ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„
3. í”„ë¡¬í”„íŠ¸ì— í’ˆì§ˆ í‚¤ì›Œë“œ ì¶”ê°€í•˜ë©´ ê²°ê³¼ ê°œì„ 
4. Negative promptë¡œ ë¶ˆì›í•˜ëŠ” íŠ¹ì„± íš¨ê³¼ì ìœ¼ë¡œ ì œê±°
5. ê°™ì€ seedë¡œ ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ ìƒì„±
```

---

## ğŸ“¦ ì„¤ì¹˜ ë° ì‹¤í–‰ ê°€ì´ë“œ

### ìµœì†Œ ì„¤ì¹˜ (Chapter 1-2)
```bash
pip install nltk scikit-learn pandas sentence-transformers torch transformers
```

### ì „ì²´ ì„¤ì¹˜ (ëª¨ë“  Chapter)
```bash
pip install nltk scikit-learn pandas sentence-transformers torch transformers langchain pillow diffusers
```

### ì²« ì‹¤í–‰
```bash
# Chapter 1
python llm_1_1_text_preprocessing.py
python llm_1_2_tfidf_vectorization.py
python llm_1_3_word_embeddings.py

# Chapter 2
python llm_2_1_lstm_sentiment.py
python llm_2_2_bert_sentiment.py
```

---

## ğŸ“ ì™„ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸

### Chapter 1 (í…ìŠ¤íŠ¸ ë°ì´í„° í‘œí˜„) - 3ê°œ
- [ ] ë¬¸ì œ 1.1: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
- [ ] ë¬¸ì œ 1.2: TF-IDF ë²¡í„°í™”
- [ ] ë¬¸ì œ 1.3: ë‹¨ì–´ ì„ë² ë”©

### Chapter 2 (ìì—°ì–´ ë”¥ëŸ¬ë‹) - 2ê°œ
- [ ] ë¬¸ì œ 2.1: LSTM ê°ì„± ë¶„ì„
- [ ] ë¬¸ì œ 2.2: BERT ì „ì´í•™ìŠµ

### Chapter 3 (ì´ˆê±°ëŒ€ ì–¸ì–´ ëª¨ë¸) - 3ê°œ
- [ ] ë¬¸ì œ 3.1: GPT í…ìŠ¤íŠ¸ ì™„ì„±
- [ ] ë¬¸ì œ 3.2: ìƒì„± íŒŒë¼ë¯¸í„° ì œì–´
- [ ] ë¬¸ì œ 3.3: ë§ˆì¼€íŒ… ì¹´í”¼ ìƒì„±

### Chapter 4 (í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§) - 3ê°œ
- [ ] ë¬¸ì œ 4.1: Few-Shot Learning
- [ ] ë¬¸ì œ 4.2: LangChain RAG
- [ ] ë¬¸ì œ 4.3: Chain-of-Thought

### Chapter 5 (íŒŒì¸íŠœë‹) - 2ê°œ
- [ ] ë¬¸ì œ 5.1: LoRA íŒŒì¸íŠœë‹
- [ ] ë¬¸ì œ 5.2: RLHF ë³´ìƒ ëª¨ë¸

### Chapter 6 (ë©€í‹°ëª¨ë‹¬) - 2ê°œ
- [ ] ë¬¸ì œ 6.1: ì´ë¯¸ì§€ ìº¡ì…”ë‹
- [ ] ë¬¸ì œ 6.2: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±

---

## ğŸ’¡ ì¶”ê°€ í•™ìŠµ ìë£Œ

- [Hugging Face ê³µì‹ íŠœí† ë¦¬ì–¼](https://huggingface.co/learn)
- [Fast.ai NLP ê°•ì¢Œ](https://www.fast.ai/)
- [OpenAI í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°€ì´ë“œ](https://platform.openai.com/docs/guides/prompt-engineering)

---

**Last Updated:** 2025ë…„ 10ì›” 27ì¼  
**Status:** âœ… COMPLETE (ì´ 15ê°œ ë¬¸ì œ)
