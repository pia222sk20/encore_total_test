# 데이터 분석, 머신러닝, 딥러닝 종합 실습 문제 - 개선판

> **목표:** 학생들이 명확한 지시사항을 따라 동일한 수준의 코드를 작성하고 일괄적으로 평가할 수 있도록 설계

## 목차
- [Section 1: NumPy 기초](#section-1-numpy-기초)
- [Section 2: Pandas 데이터 처리](#section-2-pandas-데이터-처리)
- [Section 3: 데이터 시각화](#section-3-데이터-시각화)
- [Section 4: 머신러닝](#section-4-머신러닝)
- [Section 5: 딥러닝](#section-5-딥러닝)
- [Section 6: 모델 평가](#section-6-모델-평가)

---

## Section 1: NumPy 기초

### 문제 1.1: NumPy 벡터 및 행렬 연산

**지시사항:**

1. 다음 벡터를 생성하고 내적 계산
   - v1 = [1, 2, 3, 4], v2 = [5, 6, 7, 8]
   - np.dot() 또는 @ 연산자 사용
   - 결과 출력 및 수동 검증

2. 행렬 곱셈 수행
   - 2×3 행렬 A와 3×2 행렬 B 생성
   - A @ B 계산 (결과: 2×2 행렬)

3. 벡터 노름 계산
   - 벡터 c = [3, 4, 5]에 대해
   - L1, L2, L∞ 노름 모두 계산
   - np.linalg.norm() 사용

4. 조건부 인덱싱
   - 배열 data = [1, 5, 3, 8, 2, 9, 4, 7, 6]
   - 5보다 큰 값 추출
   - 3 이상 7 이하 값 추출
   - 짝수 값 추출

**필수 라이브러리:**
```bash
pip install numpy
```

**평가 기준:**
- 모든 연산 결과 정확성 ✓
- 조건부 인덱싱 정확성 ✓
- 코드 가독성 ✓

**파일명:** `problem_1_1_numpy_operations.py`

---

### 문제 1.2: NumPy 브로드캐스팅

**지시사항:**

1. 스칼라와 배열 연산
   ```
   arr = [1, 2, 3, 4]
   arr에 10을 더한 결과 출력
   ```

2. 1D × 2D 배열 연산
   ```
   arr2d = [[1,2,3], [4,5,6], [7,8,9]] (3×3)
   arr1d = [10, 20, 30] (1×3)
   arr2d + arr1d 계산 (각 행에 arr1d 더함)
   ```

3. 열 벡터 × 행 벡터 연산
   ```
   col = [[1], [2], [3]] (3×1)
   row = [10, 20, 30, 40] (1×4)
   col + row 계산 (결과: 3×4)
   ```

4. 실제 응용: 거리 행렬 계산
   - 포인트 A = [[0,0], [3,4]]
   - 포인트 B = [[0,0], [3,4], [5,5]]
   - A와 B 사이 유클리드 거리 행렬 계산
   - (브로드캐스팅으로 반복문 없이 구현)

**필수 라이브러리:**
```bash
pip install numpy
```

**파일명:** `problem_1_2_numpy_broadcasting.py`

---

## Section 2: Pandas 데이터 처리

### 문제 2.1: Pandas DataFrame 기초

**지시사항:**

1. DataFrame 생성 (직원 정보)
   ```
   columns: 이름, 나이, 도시, 급여, 부서
   rows: 최소 5명 이상의 직원 데이터
   ```

2. 기본 정보 확인
   - df.info() 실행
   - df.describe() 실행
   - 각 열의 데이터 타입 확인

3. 단일 조건 필터링
   - 나이 > 30인 직원
   - 급여 >= 4000인 직원
   - 특정 도시 직원

4. 복합 조건 필터링
   - (도시 == '서울') & (급여 >= 4000)
   - (부서 == '개발') | (급여 >= 4500)

5. 통계 계산
   - 부서별 평균 급여
   - 도시별 직원 수
   - 나이의 기본통계

**필수 라이브러리:**
```bash
pip install pandas
```

**평가 기준:**
- 정확한 필터링 결과 ✓
- 올바른 통계 계산 ✓
- 코드 구조화 ✓

**파일명:** `problem_2_1_pandas_dataframe.py`

---

### 문제 2.2: Pandas GroupBy 분석

**지시사항:**

1. Titanic 데이터 로드
   ```python
   import seaborn as sns
   df = sns.load_dataset('titanic')
   ```

2. 좌석등급별 생존율 계산
   - groupby('pclass')['survived'].mean()
   - 결과 3개 그룹 출력

3. 성별×좌석등급별 집계
   - groupby(['sex', 'pclass']).agg() 사용
   - 각 그룹별 나이(평균), 요금(합계), 생존자 수 계산

4. 피벗 테이블 생성
   - 행: sex, 열: pclass, 값: 생존율
   - 결과를 표로 출력

5. 최고 생존율 그룹 찾기
   - 어느 그룹의 생존율이 가장 높은지?

**필수 라이브러리:**
```bash
pip install pandas seaborn
```

**평가 기준:**
- 정확한 그룹화 ✓
- 올바른 집계 함수 사용 ✓
- 결과 해석 ✓

**파일명:** `problem_2_2_pandas_groupby.py`

---

### 문제 2.3: 결측값 처리

**지시사항:**

1. 결측값 현황 파악
   - Titanic 데이터 사용
   - 각 열별 결측값 개수 계산
   - 결측값 비율(%) 계산

2. 전략 1: 삭제
   - Age 열의 결측값을 dropna()로 제거
   - 삭제 전후 행 수 비교

3. 전략 2: 대체 - 단순 통계값
   - 평균값으로 대체
   - 중앙값으로 대체
   - 최빈값으로 대체

4. 전략 3: 대체 - 그룹별 대체
   - 성별별 평균으로 대체
   - 좌석등급별 평균으로 대체
   - 성별 + 좌석등급별 평균으로 대체

5. 결과 비교
   - 각 전략별 결측값 처리 후 데이터 확인
   - 각 전략의 장단점 기술

**필수 라이브러리:**
```bash
pip install pandas seaborn
```

**파일명:** `problem_2_3_missing_values.py`

---

### 문제 2.4: 이상치 탐지

**지시사항:**

1. 이상치 탐지 방법 1: IQR
   - Titanic의 Fare 열 사용
   - Q1, Q3, IQR 계산
   - 이상치 범위 결정 (Q1-1.5×IQR, Q3+1.5×IQR)
   - 이상치 개수 출력

2. 이상치 탐지 방법 2: Z-score
   - Z-score 계산: (x - mean) / std
   - |Z-score| > 2.5인 값을 이상치로 분류
   - 이상치 개수 출력

3. 두 방법 비교
   - 같은 이상치를 탐지했는가?
   - 각 방법의 장단점

4. 이상치 처리
   - 이상치를 75 백분위수로 상한선 설정 (Capping)
   - 처리 전후 통계값 비교

**필수 라이브러리:**
```bash
pip install pandas seaborn scipy
```

**평가 기준:**
- 정확한 이상치 탐지 ✓
- 올바른 처리 방법 ✓

**파일명:** `problem_2_4_outlier_detection.py`

---

### 문제 2.5: 데이터 스케일링

**지시사항:**

1. 스케일링 필요성 이해
   - Iris 데이터 로드
   - 각 특성의 범위 확인 (min, max)
   - 특성 간 스케일 차이 확인

2. StandardScaler 적용
   ```python
   from sklearn.preprocessing import StandardScaler
   scaler = StandardScaler()
   X_scaled = scaler.fit_transform(X)
   ```
   - 평균과 표준편차 확인 (평균≈0, 표준편차≈1)

3. MinMaxScaler 적용
   ```python
   from sklearn.preprocessing import MinMaxScaler
   scaler = MinMaxScaler()
   X_scaled = scaler.fit_transform(X)
   ```
   - 범위가 [0, 1]로 정규화되는지 확인

4. 성능 비교
   - 스케일링 없이 KNN 모델 성능 측정
   - StandardScaler 적용 후 성능 측정
   - MinMaxScaler 적용 후 성능 측정
   - 성능 개선 비율 계산

**필수 라이브러리:**
```bash
pip install numpy pandas scikit-learn
```

**파일명:** `problem_2_5_data_scaling.py`

---

## Section 3: 데이터 시각화

### 문제 3.1: Matplotlib 기초

**지시사항:**

1. 선 그래프 그리기
   - x = [1, 2, 3, 4, 5]
   - y = [1, 4, 9, 16, 25]
   - 그래프 제목, x축 레이블, y축 레이블 추가

2. 산점도 그리기
   - 랜덤 데이터 생성 (점 100개)
   - 색상 다르게 표현
   - 범례 추가

3. 히스토그램
   - Iris 데이터의 꽃잎 길이 분포
   - bin 개수 30으로 설정
   - 제목과 축 레이블 추가

4. 여러 그래프 한 화면에
   - subplot(2, 2) 사용
   - 4개 Iris 특성별 히스토그램

**필수 라이브러리:**
```bash
pip install matplotlib numpy scikit-learn
```

**파일명:** `problem_3_1_matplotlib_visualization.py`

---

### 문제 3.2: Seaborn 고급 시각화

**지시사항:**

1. 히트맵 그리기
   - Iris 상관계수 행렬 계산
   - seaborn의 heatmap() 사용
   - 주석(annot=True) 추가

2. 박스플롯
   - Iris 데이터를 품종별로 분류
   - sepal_length 박스플롯 그리기
   - 이상치 확인

3. 바이올린 플롯
   - Iris 데이터를 품종별로 분류
   - petal_length 바이올린 플롯
   - 분포 비교

4. 페어플롯
   ```python
   sns.pairplot(iris, hue='species')
   ```
   - Iris의 모든 특성 쌍 비교

**필수 라이브러리:**
```bash
pip install seaborn pandas scikit-learn matplotlib
```

**파일명:** `problem_3_2_seaborn_visualization.py`

---

## Section 4: 머신러닝

### 문제 4.1: 선형 회귀

**지시사항:**

1. 데이터 생성
   - X: 1~100 (100개)
   - y = 2*X + 5 + 노이즈(표준편차 20)

2. 모델 학습
   ```python
   from sklearn.linear_model import LinearRegression
   model = LinearRegression()
   model.fit(X.reshape(-1,1), y)
   ```

3. 예측 및 평가
   - 새로운 x값 [50, 75, 150]에 대해 y 예측
   - 학습 데이터로 평가: MSE, RMSE, R² 계산

4. 시각화
   - 산점도로 데이터 표시
   - 회귀선 그리기
   - 예측선과 실제 데이터 비교

**필수 라이브러리:**
```bash
pip install scikit-learn numpy matplotlib
```

**평가 기준:**
- 정확한 모델 구현 ✓
- 올바른 성능 평가 ✓

**파일명:** `problem_4_1_linear_regression.py`

---

### 문제 4.2: 로지스틱 회귀 (이진 분류)

**지시사항:**

1. Iris 데이터 준비
   - 품종을 'setosa' vs 'not setosa'로 이진 분류
   - 특성은 모든 4개 사용

2. 모델 학습
   ```python
   from sklearn.linear_model import LogisticRegression
   model = LogisticRegression()
   model.fit(X_train, y_train)
   ```

3. 예측 및 평가
   - 테스트 데이터 예측
   - 정확도(accuracy) 계산
   - 혼동 행렬 출력
   - 정밀도, 재현율, F1-score 계산

4. 시각화
   - 혼동 행렬 히트맵

**필수 라이브러리:**
```bash
pip install scikit-learn pandas numpy matplotlib seaborn
```

**파일명:** `problem_4_2_logistic_regression.py`

---

### 문제 4.3: 의사결정 트리 (분류)

**지시사항:**

1. 모델 학습
   ```python
   from sklearn.tree import DecisionTreeClassifier
   model = DecisionTreeClassifier(max_depth=3, random_state=42)
   model.fit(X_train, y_train)
   ```

2. 예측 및 평가
   - 정확도 계산
   - 특성 중요도 출력 (어느 특성이 가장 중요한가?)

3. 트리 시각화
   ```python
   from sklearn.tree import plot_tree
   plt.figure(figsize=(20, 10))
   plot_tree(model, feature_names=[...], class_names=[...], filled=True)
   ```

4. 성능 비교
   - max_depth를 1, 3, 5, 10으로 변경
   - 각 depth별 정확도 비교
   - 과적합 확인

**필수 라이브러리:**
```bash
pip install scikit-learn pandas numpy matplotlib
```

**파일명:** `problem_4_3_decision_tree.py`

---

### 문제 4.4: SVM (서포트 벡터 머신)

**지시사항:**

1. 모델 학습
   ```python
   from sklearn.svm import SVC
   model = SVC(kernel='rbf', C=1.0)
   model.fit(X_train, y_train)
   ```

2. 예측 및 평가
   - 정확도 계산
   - 혼동 행렬 출력

3. 커널 비교
   - kernel='linear', 'rbf', 'poly' 비교
   - 각 커널별 정확도 계산

4. C 파라미터 영향
   - C=[0.1, 1, 10, 100] 비교
   - 각 C값별 정확도 계산
   - 최적 C값 찾기

**필수 라이브러리:**
```bash
pip install scikit-learn pandas numpy
```

**파일명:** `problem_4_4_svm.py`

---

### 문제 4.5: K-Means 클러스터링

**지시사항:**

1. 데이터 생성
   - 3개 클러스터로 구성된 합성 데이터 생성
   - 100개 포인트

2. 모델 학습
   ```python
   from sklearn.cluster import KMeans
   model = KMeans(n_clusters=3, random_state=42)
   labels = model.fit_predict(X)
   ```

3. 시각화
   - 클러스터별 다른 색상으로 표시
   - 중심(centroid) 표시

4. 최적 클러스터 수 찾기
   - k=1~10 범위에서 엘보우 방법 적용
   - 관성(inertia) 값 계산 및 그래프

**필수 라이브러리:**
```bash
pip install scikit-learn numpy matplotlib
```

**파일명:** `problem_4_5_kmeans.py`

---

### 문제 4.6: 고객 세분화 (K-Means 실제 응용)

**지시사항:**

1. 데이터 준비
   - 온라인 소매 데이터셋 로드
   ```python
   df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.csv', encoding='unicode_escape')
   ```
   - 결측값 제거
   - 수량과 단가가 0 이상인 데이터만 필터링

2. 고객별 구매 통계 계산
   - 각 고객의 총 구매금액(RFM의 M)
   - 각 고객의 거래 횟수(RFM의 F)

3. 데이터 스케일링
   - StandardScaler 적용

4. K-Means 클러스터링
   - k=3으로 고객 세분화
   - 클러스터별 특성 분석

5. 결과 분석
   - 각 클러스터별 평균 구매금액
   - 각 클러스터별 평균 거래 횟수
   - 클러스터별 고객 수

**필수 라이브러리:**
```bash
pip install pandas numpy scikit-learn
```

**파일명:** `problem_4_6_customer_segmentation.py`

---

## Section 5: 딥러닝

### 문제 5.1: 신경망 기초 (손실함수와 역전파)

**지시사항:**

1. 간단한 신경망 구현
   ```python
   import tensorflow as tf
   from tensorflow import keras
   ```

2. 데이터 준비
   - MNIST 데이터셋 로드
   - 픽셀 값을 0~1 범위로 정규화
   - 훈련/테스트 분할

3. 모델 구축
   - Input: 784 (28×28)
   - Dense(128, activation='relu')
   - Dense(10, activation='softmax')
   - 손실함수: categorical_crossentropy
   - 옵티마이저: adam

4. 모델 학습
   - 에포크 10, 배치 크기 32
   - 훈련 중 정확도 모니터링

5. 평가
   - 테스트 정확도 출력
   - 처음 10개 테스트 이미지에 대한 예측 결과 출력

**필수 라이브러리:**
```bash
pip install tensorflow numpy
```

**파일명:** `problem_5_1_neural_network.py`

---

### 문제 5.2: 손실함수와 역전파 이해

**지시사항:**

1. 손실함수 계산
   - 실제값: y = [0, 1, 0]
   - 예측값: y_pred = [0.1, 0.8, 0.1]
   - Cross-entropy loss 수동 계산

2. 수치 미분으로 그래디언트 계산
   ```python
   # 간단한 함수: f(x) = x^2
   # x=3에서 그래디언트 계산 (약 6)
   ```

3. 역전파 이해
   - 간단한 계산 그래프: f(x) = w*x + b
   - ∂f/∂w, ∂f/∂x 계산

4. 신경망에서의 역전파
   - 2층 신경망에서 가중치 업데이트 과정 시각화

**필수 라이브러리:**
```bash
pip install numpy tensorflow
```

**파일명:** `problem_5_2_backpropagation.py`

---

### 문제 5.3: 감정 분석 (NLP 활용)

**지시사항:**

1. 데이터 준비
   - 영화 리뷰 데이터셋 로드 (IMDB)
   ```python
   from tensorflow.keras.datasets import imdb
   ```

2. 데이터 전처리
   - 단어 인덱스를 실제 단어로 변환
   - 텍스트 정규화

3. 신경망 모델
   - Embedding 레이어 (차원: 16)
   - LSTM 또는 Dense 레이어
   - 출력 레이어 (이진 분류: 긍정/부정)

4. 모델 학습
   - 에포크 10
   - 검증 데이터 사용

5. 평가
   - 테스트 정확도
   - 새로운 리뷰 예측

**필수 라이브러리:**
```bash
pip install tensorflow numpy
```

**파일명:** `problem_5_3_sentiment_analysis.py`

---

## Section 6: 모델 평가

### 문제 6.1: 혼동 행렬과 성능 지표

**지시사항:**

1. 혼동 행렬 이해
   ```
   [[TN, FP],
    [FN, TP]]
   ```

2. 성능 지표 계산
   - 정확도 = (TP+TN)/(TP+TN+FP+FN)
   - 정밀도 = TP/(TP+FP)
   - 재현율 = TP/(TP+FN)
   - F1-score = 2×(정밀도×재현율)/(정밀도+재현율)

3. 이진 분류 모델 평가
   - Titanic 생존 예측 모델 구현
   - 모든 지표 계산

4. 시각화
   - 혼동 행렬 히트맵
   - ROC 곡선

**필수 라이브러리:**
```bash
pip install scikit-learn pandas numpy matplotlib seaborn
```

**평가 기준:**
- 정확한 지표 계산 ✓
- 올바른 해석 ✓

**파일명:** `problem_6_1_confusion_matrix.py`

---

### 문제 6.2: 교차 검증 (Cross Validation)

**지시사항:**

1. k-Fold 교차 검증
   ```python
   from sklearn.model_selection import cross_val_score
   scores = cross_val_score(model, X, y, cv=5)
   ```
   - k=5로 설정
   - 각 fold별 정확도 출력
   - 평균 정확도 및 표준편차 계산

2. 모델 비교
   - 로지스틱 회귀
   - 의사결정 트리
   - SVM
   - 각 모델의 평균 정확도 비교

3. 하이퍼파라미터 튜닝
   - GridSearchCV 사용
   ```python
   from sklearn.model_selection import GridSearchCV
   param_grid = {'C': [0.1, 1, 10, 100]}
   grid_search = GridSearchCV(SVC(), param_grid, cv=5)
   ```

**필수 라이브러리:**
```bash
pip install scikit-learn pandas numpy
```

**파일명:** `problem_6_2_cross_validation.py`

---

### 문제 6.3: 하이퍼파라미터 최적화

**지시사항:**

1. GridSearchCV 사용
   - RandomForestClassifier의 최적 파라미터 찾기
   - 탐색 범위:
     - n_estimators: [50, 100, 200]
     - max_depth: [5, 10, 20, None]
   - cv=5

2. 최적 파라미터 확인
   ```python
   print(grid_search.best_params_)
   print(grid_search.best_score_)
   ```

3. 최적 모델 평가
   - 테스트 데이터로 최종 평가
   - 성능 지표 계산

4. 비교 분석
   - 기본 파라미터 vs 최적 파라미터
   - 성능 개선 비율 계산

**필수 라이브러리:**
```bash
pip install scikit-learn pandas numpy
```

**파일명:** `problem_6_3_hyperparameter_tuning.py`

---

## 제출 및 평가

**모든 코드는 다음 조건을 만족해야 합니다:**

1. ✓ 지시사항의 모든 요구사항 충족
2. ✓ 코드 실행 오류 없음
3. ✓ 명확한 출력 메시지 포함
4. ✓ 적절한 변수명 사용
5. ✓ 주석으로 주요 로직 설명
6. ✓ 최종 결과값 정확성 검증

**파일 제출 형식:**
```
problem_1_1_numpy_operations.py
problem_1_2_numpy_broadcasting.py
...
problem_6_3_hyperparameter_tuning.py
```
